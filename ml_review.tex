\documentclass[11pt]{article}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{1pt}

\def\indented#1{\list{}{}\item[]}
\let\indented=\endlist


\newcounter{questionCounter}
\newcounter{partCounter}[questionCounter]
\newenvironment{question}[2][\arabic{questionCounter}]{%
    \addtocounter{questionCounter}{1}%
    \setcounter{partCounter}{0}%
    \vspace{.25in} \hrule \vspace{0.5em}%
        \noindent{\bf #1: #2}%
    \vspace{0.8em} \hrule \vspace{.10in}%
}{}
\renewenvironment{part}[1][\arabic{partCounter}]{%
    \addtocounter{partCounter}{1}%
    \vspace{.10in}%
    \begin{indented}%
       {\bf (#1)} %
}{\end{indented}}


%%%%%%%%%%%%%%%%% Identifying Information %%%%%%%%%%%%%%%%%
%% This is here, so that you can make your homework look %%
%% pretty when you compile it.                           %%
%%     DO NOT PUT YOUR NAME ANYWHERE ELSE!!!!            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\argmax}{\arg\max}
\newcommand{\argmin}{\arg\min}
\newcommand{\myname}{Daniel Howarth, Malcolm Greaves}
\newcommand{\assignment}{Final Review}
\newcommand{\course}{Final Review 10-601 Machine Learning S11}
\newcommand{\andrewid}{dhowarth, mwgreave}
\newcommand{\myandrew}{dhowarth@andrew.cmu.edu, mwgreave@andrew.cmu.edu}
\newcommand{\myhwnum}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancyplain}
\chead{\course}

\begin{document}
.
\\
\\
Created by Dan Howarth and Malcolm Greaves. Adapted significantly from ``Machine Learning'' by Tom M. Mitchell (1997).
\\
\\
Table of Contents:
\begin{itemize}
	\item Overview of Learning
	\item Concept Learning
	\item Information Theory
	\item Decision Trees
	\item Artificial Neural Networks
	\item Computational Learning Theory
	\item Evaluating and Comparing Hypotheses
	\item Bayesian Statistics and the Naive Bayes Classifier
	\item Bayes Networks and Expectation Maximization
	\item Hidden Markov Models
	\item Instance Based Learning
	\item Reinforcement Learning
	\item Genetic Algorithms
\end{itemize}
\begin{question}{Overview of learning}
\textbf{Definition} A computer program is said to \textbf{learn} from experience $E$, with respect to some class of tasks $T$ and performance measure $P$, if the program's performance at tasks in $T$, as measured by $P$, improves with experience $E$. (Mitchell, p2)
\\
\\
Machine learning is about learning the \textbf{function}--the relationship between inputs and outputs--of some underlying target concept or phenomenon. 
\end{question}
\\
\\
Machine learning in the finite case is called \textbf{classification}, and machine learning in the infinite case is called \textbf{regression}.
\\
\\
The principle questions for all machine learning algorithms:
\\
\text{	}\text{	}\textbf{Representation Power (hard bias).}\\
\text{	}\text{	}\textbf{Preference bias (soft, rank, search).}\\
\text{	}\text{	}\textbf{Search feasibility/efficiency.}\\
\text{	}\text{	}\textbf{Generalization (Will perform well on new data?)}\\\



\begin{question}{Concept Learning: Sorry, the year 1967 and there is no other option.}
Note that all of the learning algorithms in this section exhibit a \textbf{hard bias}. That is, a restriction on the hypothesis space that is necessary to successfully learn the target concept.
\\
\\
\textbf{Definition:} Inferring a boolean-valued function from training examples of its input and output is learning a \textbf{target concept}. Note that this idea can be extended to finite classes and regression.
\\
\\
\textbf{The inductive learning hypothesis} is that any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples.
\\
\\
We are able to order hypothesis on the continuum from least specific to most specific. With this ordering, we are also able to define the set of hypothesis bounded by the most and least specific hypotheses.
\\
\\
\textbf{Definition} Let $h_i, h_j$ be hypotheses defined over some set of instances $X$. $h_J$ is \textbf{more general than or equal to} $h_i$ ($h_j \geq_g h_i$)if and only if $$(\forall x \in X)(h_i(x) = 1 \implies h_j(x) = 1)$$. And, in this case, hypothesis $h_i$ is \textbf{more specific than or equal to} $h_j$ (written $h_i \leq_s h_j$).
\\
\\
Moreover, we can define two simple algorithms--Find-S and Find-G--that are able to, respectively, find the most specific and most general hypotheses.
\\
FIND-S: (FIND-G with minimal change)*\\
\text{	} Let $h = $ most specific $h \in H$ \\
\text{	} for each $x \in X$ such that $c(x) = 1$: \\
\text{	} \text{	} \text{	} for each attribute $a_i$ in $h$:\\
\text{	} \text{	} \text{	} \text{	} \text{	} if $a_i$ is satisfied by $x$ then do nothing \\
\text{	}\text{	}\text{	}\text{	}\text{	} else replace $a_i$ in $h$ with the next more general constraint that is satisfied by $x$
\\
\\
Note, to make this FIND-G, (1) initialize $h$ to be the most general hypothesis in $H$ (2) iterate over $x$ that satisfy $c(x) = 0$ (3) if $a_i$ in $h$ disagrees with $a_i$ in $x$, then change the attribute $a_i$ in $h$ to the next most specific constraint that satisfies $x$.
\\
\\
\textbf{Definition} A hypothesis $h$ is \textbf{consistent} with a set of training examples $D$ if any only if $(\forall [x,c(x)] \in D)(h(x) = c(x))$. That is, $h$ classifies all instances correctly.
\\
\\
\textbf{Definition:} The \textbf{version space} of a hypothesis space $H$ with respect to a set of training examples $D$ (denoted $VS_{H.D}$) is the subset of hypothesis that are consistent with $D$. $$VS_{H,D} = \{ h \in H | Consistent(h,D) \}$$
\\
\\
We have two ways of representing the version space. We can either explicitly or implicitly represent the space. 
\\
\\
\text{	} \text{	} To explicitly represent $VS_{H,D}$, we can \text{list} every $h \ in H$, and \textbf{then}, for each training example, \text{eliminate} all hypothesis that are inconsistent with the example. The remaining hypotheses are precisely the hypotheses in $H$ consistent with $D$, and thus is a complete list of the version space.
\\
\\
\text{	} \text{	} To implicitly represent the version space, we can keep track of the most specific and most general hypothesis in $H$. These two hypotheses, $h_s$ and $h_g$, bound the set of consistent hypothesis, thus delimiting the version space. However, there may be more than one member of $H$ that is maximally general or specific. Thus, we formally define these two boundaries of the version space. The \textbf{candidate elimination} algorithm, which implicitly defines $VS_{H,D}$, is essentially running FIND-S and FIND-G in parallel, while keeping track of the general and specific boundaries (defined below).
\\
\\
\textbf{Definition} The \textbf{general boundary} $G$ of a hypothesis space $H$ and training data $D$ is the set of maximally general members of $H$ consistent with $D$ $$G = \{ g \in H | Consistent(g,D) \wedge (\not \exists h \in H)(h >_g g \wedge Consistent(h,D)) \}$$
\\
\\
\textbf{Definition} The \textbf{specific boundary} $S$ of a hypothesis space $H$ and training data $D$ is the set of maximally specific members of $H$ consistent with $D$ $$S = \{ s \in H | Consistent(s,D) \wedge (\not \exists h \in H)(h >_s s \wedge Consistent(h,D)) \}$$
\end{question}
\\
\\
It is worthy to note that \textbf{candidate elimination} will converge toward a hypothesis that correctly describes the target concept if there are no errors in the training examples and if there exists a hypothesis in $H$ that correctly describes the target concept. If either of these two conditions are not fulfilled, then candidate elimination will output the empty set as the version space (effectively learning that there is no hypothesis that captures the target concept).
\\
\\
One effective use of using candidate elimination to define a version space is to use the set of hypothesis output by candidate elimination in a voting scheme, where all hypotheses are ``asked'' to give the classification of an unobserved instance $x$, and the majority decision of all hypotheses is taken as the classification of $x$. Of course, non-equal weightings of hypotheses and different voting schemes may be used to construct a classifier in this manner.
\\
\\
\underline{The Importance of Bias in Learning}\\
Consider having an unbiased learner $U$. That is, the hypothesis space $H$ for $U$ is the power set of all instances $X$. $H$ has no bias because $H$ includes every possible classification of instances. However, since $H$ has no bias, $U$ will only be able to \textbf{memorize} the classification of instances it observes: $U$ has absolutely no ability to generalize beyond observed training instances. The reason why this is the case is because, after observing any instance $x$, exactly half of the hypotheses in $H$ will agree with the classification of $x$ and the other half will disagree with the classification of $x$.
\\
\\
One can think of unbiased learning as a problem of indecisiveness.
\\
\\
\textbf{Definition}: Let $L$ be a concept learning algorithm with hypothesis space $H$ for the set of instances $X$. Let $D_c = \{ <x,c(x)>\}$ be some set of training examples for concept $c$. $L(x, D_c)$ is the classification of instance $x$ by learner $L$ after training on data $D_c$. The \textbf{inductive bias} of $L$ is any minimal set of assertions $B$ that we assume to be true (to justify our inductive inferences as deductive inferences) such that for any target concept $c$ and corresponding training examples $D_c$ $$(\forall x \in X)[(B \wedge D_c \wedge x) \vdash L(x,D_c)]$$


\begin{question}{Information Theory}

\underline{Information:}
$I(E) = log_2 \frac{1}{P(E)}$

Think of information as a level uncertainty about an event. If a probability is very low then the information it gives is high.

Information is additive. The information in single coin flip is 1 bit. In k coin flips there is k bits of information.
Intuitively, adding logs is the same as multiplying their paramters.

\underline{Entropy:}

$H(S) = \displaystyle\sum\limits_i p_i I(s_i)$ 

Can take integral for continuous functions.

Properties:
\begin{itemize}
    \item Entropy is non-negative.
    \item Invariant with respect to order of inputs
    \item The further the distribution is from uniform, the lower the entropy.
\end{itemize}

\underline{Joint Entropy:}

$H(A,B) = \displaystyle\sum\limits_{a,b} P(A=a, B=b)\log\frac{1}{P(A=a, B=b)}$

$H(A,B) < H(A) + H(B)$ TODO: Is this only when dependent?

\underline{Conditional Entropy:}


$H(A/B) = \displaystyle\sum\limits_{b}P(B=b)H(A|B=b)$

We can ask ``How much is B telling us about A'' and we get...

\underline{Mutual Information}

``'';'' means between

$I(X;Y) = H(X) - H(X|Y) = H(Y)-H(Y|X) = H(X) + H(Y) - H(X,Y) = H(X,Y) - H(X|Y) - H(Y|X)$
\\
$I(X;Y) = \displaystyle\sum\limits_{y \in Range(Y)}{\displaystyle\sum\limits_{x \in Range(X)}{p_{X,Y}(x,y) \log_2 \frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}}}$
\\
\\
$I(X;Y;Z) = I(X;Y) - I(X;Y/Z) = H(X,Y,Z) - H(X,Y) - H(X,Z) - H(Y,Z) + H(X) + H(Y) + H(Z)$
\\
\\
Note that $I(X;X) = H(X) - H(X|X) = H(X)$

Can be negative with three variables.

$I(X;Y) = 0$ and $I(X;Z) = 0$ does not imply $I(X;Y;Z) = 0$



\end{question}


\begin{question}{Decision Trees: Duuuuuude... we don't have to restrict the hypothesis space!}

The \textbf{inductive bias} of decision trees is a preference for small trees over larger trees.

Decision trees are at the core a \textbf{top-down, greedy search} through the space of possible decision trees.

Appropriate for:
\begin{itemize}
    \item Instances are rep by attribute-value pairs (as vectors)
    \item The target function has discrete output value pairs.
    \item Disjunctive descriptions may be required.
    \item The training data may contain errors
    \item The training data may contain missing attribute values
    \item Train attributes with different costs
    \item When you do not want to risk ``throwing the baby out of the water''
\end{itemize}

\underline{Limitations:}
Pure form of ID3 performs no back tracking when searching. This is susceptible in the same way
that all hill climb searches without back tracking to converge to a locally optimal solutions that are not
globally optimal

\underline{Capabilities:}

ID3 uses all training examples at each step to perform a statistical analysis of how to
refine it's current hypothesis. An advantage of this is that the resulting search is less
sensitive to errors and noise in individual data. 

\textbf{Representation Power (hard bias):}

The hypothesis space of all decision trees is a \emph{complete} space of finite discrete-valued
functions, relative to the available attributes. 

It only keeps track a single current hypothesis when searching the space of DTs. Cannot keep track 
of all consistent hypotheses.

\textbf{Preference bias (soft, rank, search):}

The \textbf{inductive bias} of decision trees is a preference for small trees over larger trees. Trees
that place high information gain attributes near the root are preferred over those that do not.

\underline{Occam's razor:} Prefer the simplest hypothesis that fits that data.

\textbf{Search feasibility/efficiency:}

\textbf{Generalization (Will perform well on new data?)}


At each level the tree splits on the attribute that provides the max information gain.

$S_v = \{s\in S | A(s) = v\}$ is the subset of $S$ for which the attribute $A$ has value $v$.

$Gain(S,A) = Entropy(S) - \displaystyle\sum\limits_{v \in Values(A)} \frac{|S_v|}{|S|} Entropy(S_v)$

\underline{Definition:} Given a hypothesis space $H$, a hypothesis $h \in H$ is said to \textbf{overfit}
the data if $\exists h\prime \in H$, $h \ne h\prime$, such that $h$ has smaller error over the
training examples but $h\prime$ has smaller error over the entire distribtion of instances.

A DT can overfit the data even if the data is noise free. It is possible for \underline{coincidental regularities} to occur
in which some attribute happens ot partition the examples very well, despite being unrealted to the actual target
function. This happens more frequently when there are a small number of examples. 

To avoid over fitting you can do (1) stop growing the tree earlier, before it reaches the point where it perfectly
classifies the training data or (2) allow the tree to overfit the data, but then post-prune the tree.

The latter has been found to perform better in practice.

Ways to determine the correct final tree size: (1) Use a separate set of examples, distinct from the training
examples, to evaluate the utility of post-pruning nodes from the tree. (2) Use all the available data for training,
but apply statistical analysis to estimate whether expanding (or pruning) a particular node is likely
to produce a better hypothesis

\underline{Reduced error pruning:}

Pruning a node consists of first removing the subtree at that node and assigning it the most common
classification of the training examples affiliated with that node. Those nodes that are removed are
those that do no worse on the pruning set than with the original.

Nodes are pruned iteratively, always choosing the node whose removal most increases the accuracy over the
validation set. Pruning is continued until harmful.

Pruning can be done top-down or bottom-up.

A drawback of pruning is that you have to give up some training data to be part of the validation set.

\underline{Rule post pruning:}

Each node in a DT represents a conjunction. Rule post pruning takes each conjunction and prunes any preconditions
whose removal improves accuracy. After this is done for each rule, sort the rules by their estimated accuracy.

The advantages:
\begin{itemize}
    \item Can consider to prune a rule instead of a subtree.
    \item In decision trees when we consider removing nodes near the root it requires messy bookkeeping issues
    such as how to restructure the tree after removing the node while also retaining the structure of the 
    subtree below this test.
    \item Converting to rules improves human readability/understandability.
\end{itemize}

We can incorporate \textbf{continuous valued functions} be creating classifications such as whether or not
the number is below or above a certain number. These number are also chosen for their information gain.

\underline{Alternative measure for selecting attributes:}
There is a natural bias in the information gain measure that favors attributes with many values over
those with few values. Consider if Date was an attribute. It would pretty much always provide the most information.
This is because since there are so many (365) possible values it is much more probable for the attribute
to be able perfectly predict the target attribute over the training data.

A way to avoid this is to change the gain measure. If we incorporate split information (entropy):

$$SplitInformation(S,A) = - \displaystyle\sum\limits_{v\in values(A)}\frac{|S_v|}{|S|}\log_2\frac{|S_v|}{|S|}$$

This is the same thing the entropy of $S$ with respect to the values of attribute $A$. We can now use this
to give a penalty to those attributes that have uniform distributions:

$$GainRatio(S,A) \equiv \frac{Gain(S,A)}{SplitInformation(S,A)}$$


\underline{Handling training examples with missing attribute values:}

A simple method is to assign the missing attribute value to the value that is most common among
the training examples at that node. We could also assign it the value that is most common among
the training examples at that node with classification $c(x)$.

Another, more complex method is to assign a probability to each value rather than to the most common.
If $A(x) = 0$ with probability $.4$ and $A(x) = 1$ with probability $.6$ then assign the respective fractions
of the example to each value. These fractions are used for computing information gain.

\underline{Handling attributes with differing costs:}

This is useful when some attributes are costly the evaluate. For example when classifying symptoms we would not
want the first decision to be based off of an expensive lab test. To penalize for cost we can modify the gain
function:

$$\frac{Gain^2(S,A)}{Cost(A)}$$

or maybe possibly

$$\frac{2^{Gain(S,A)}-1}{(Cost(A) + 1)^w}$$

This introduces a \textbf{bias} towards lost cost attributes near the root of the tree. Which biases the search to 
low cost attributes.





\end{question}



\begin{question}{Artificial Neural Networks: }

\textbf{Appropriate for:}

\begin{itemize}
    \item Instance are represented by many attribute-value pairs (Can be rep as a vector) 
    \item The target value may be discrete valued, continuous, or a combination of multiple
    \item Training data may be noisy or contain errors
    \item Learning time isn't a concern
    \item Quick evaluation
    \item Understandability of underlying decision isn't a concern
    \item Multilayer networks are awesome for finding features in the data that a human would not pick up on. These
    features are implicit in the data.
\end{itemize}

\textbf{Representation Power (hard bias):}
Much is unknown about which classes of functions can be described with which types of networks.

\begin{itemize}
    \item Boolean function: Every boolean function can be represented with  two layers
    \item Continuous functions: Can be approximated with arbitrary low error with two layers
    \item Arbitrary functions: approximated with arbitrary accuracy with two layers
\end{itemize}

In all, we see that networks with small depths provide a very expressive hypothesis space.

\textbf{Preference bias (soft, rank, search):}

Search bias: Error gradient search.

Inductive bias: Smooth interpolation between data points. In words that make sense: If two training
examples are positive and have no negative examples between them then BP will classify instances
between as positive as well.

\textbf{Search feasibility/efficiency:}

\textbf{Generalization (Will perform well on new data?)}

\textbf{Perceptron training rule:}

Proven to converge to a consistent hypothesis given that the training examples are 
linearly separable, and a sufficiently small $\eta$ is used.

Update rule: $w_i \leftarrow w_i + \eta (t-o)x_i$

\textbf{Gradient Descent:}

Used when:
\begin{itemize}
    \item Hypothesis space contains continuously parameterized hypotheses
    \item The error can be differentiated with respect to the hypothesis parameters
\end{itemize}

Practical difficulties:
\begin{itemize}
    \item Can be pretty slow (can take thousands of iterations to converge)
    \item If there are multiple local minimum then it is not guaranteed to converge
\end{itemize}

If training data is not linearly separable it is proven to converge to best-fit
approximation.

In general: $\Delta w_i = - \eta \frac{\delta E}{\delta w_i}$


\underline{Standard GD:} 

Update rule: $w_i  \leftarrow w_i + \eta \sum_d (t_d - o_d)x_i$

Proven to converge because there is only one global minimum.

\underline{Stochastic GD update rule:}

Error defined as: $E_d = \frac{1}{2}(t_d - o_d)^2$

Update rule: $w_i \leftarrow w_i + \eta(t_d - o_d)x_i$

By taking $\eta$ sufficiently stochastic GD can be made to approximate the true
gradient descent arbitrarily closely.

Difference between Standard and Stochastic
\begin{itemize}
    \item Standard sums over all examples, stochastic only one example per time 
    \item Summing over all takes more computational time. However with standard we
    can use a larger $\eta$ (step size) because it calculates the true gradient.
    \item Stochastic is less prone to fall into local minimum because it uses various
    $E_d(w)$
\end{itemize}

\textbf{Backpropagation:}

\underline{English approximation of algorithm:}\\
For each training example apply it to the network:
\\ \text{	} \text{	} \text{	} Calculate the error, the difference in the label and the output of the network, for each of the output units. Then, backpropagate this error signal through the network, determining the error of each hidden unit (the partial derivative of the error with respect to each weight). Once we have calculated how much error each weight contributes to the total output, we will have our gradient: a vector of partial derivatives of the error with respect to each weight. We adjust all of the weights such that we ``move'' in the direction that minimizes the entire error of the network.
\\
Iterate this process until convergence (or until the error is acceptable).

\underline{Caveats:}\\
BP can have multiple local minimum. 

\underline{Sigmoid function}

$\sigma(y) = \frac{1}{1+e^-y}$

$\frac{\delta \sigma(y)}{\delta y} = \sigma(y)(1-\sigma(y)$

If we take the sigmoid function and multiply y by a constant k, we can vary
the steepness of the function.

$E(w) = \frac{1}{2}\displaystyle\sum\limits_{d} \displaystyle\sum\limits_{k\in outputs} (t_kd - o_kd)^2$

\underline{Update rules:}

Unit $j$: $w_i \rightarrow w_i + \eta \delta_j x_{ji}$

$\delta_j$ is the error term for unit $j$

Error term for output unit k: $\delta_k  = o_k(1 - o_k)(t_k - o_k)$

Error term for hidden unit j: $\delta_j =  o_j(1-o_j) \displaystyle\sum\limits_{l \in DS}\delta_lw_{lj}$\\
For hidden units think of it as the degree to which the unit is responsible for the error.

\underline{Momentum}

Update rule: $\Delta w_{ji}(n) = \eta \delta_j x_{ji} + \alpha \Delta w_{ji}(n-1)$

What's it do?
\begin{itemize}
    \item Is the gradient unchanging? Well then let's hurry this thing up.
    \item Keeps it moving in the same direction, which can keep the ball rolling through local minima
    \item If the gradient is flat then you wouldn't know where to go. Momentum keeps it rollin'
\end{itemize}

\underline{Convergence and god damn local minima:}

BP is not guaranteed to converge to local minima.

For high dimensional networks what may be a local min for one weight is probably not a local
min with respect to another weight. So as there are more weights, there are more ``escape'' routes
in which we can get out of local minimum. 

When the weights are initialized they are close to zero. At these points the sigmoid function
acts near linearly. This allows for a much smoother gradient surface when the algorithm starts out. 
When the weights are much different the surface is much more complex. These initial conditions
prove to be very useful because we hope that by the time the weights reach this point we hope
that the algorithm has already moved to a location near the global minimum, and local minimum
at this point are acceptable.

\underline{Heuristics to stay out of local min:}
\begin{itemize}
    \item Momentum
    \item Stochastic over Standard. Each training example has a different error gradient, and each one
    can have different local minimum, which makes it harder to get stuck in any one of them.
    \item Training multiple networks, but starting with different weights. This is basically just
    starting at different parts on the gradient surface. Choose best performer. Or you could use
    the different networks as voters for the output.
\end{itemize}


\underline{Hidden layer representations:}
Hidden layers learn/invent features that the human designer did not put into the algorithm. The higher
number of layers the higher the complexity of the features.

Remember the example of taking 8 binary strings and mapping them to themselves. This was able to be done
with a single hidden layer of 3 units. Un-surprisingly enough $2^3$ is eight.

\underline{Stopping Criterion, Overfitting, and generalization:}

One idea is to stop when error drops below a threshold. However this ends up being a poor stopping criterion
because it is very susceptible to overfitting.

Generalization Accuracy - Accuracy at which the network fits examples beyond the training data

Generalization accuracy get worse over while the training error decreases because the network
can be tuned to the noisy or idiosyncrasies of the training examples, and not tuned to the general
distribution of instances. The larger the number of weights the more degree of freedom the network
has to fit these idiosyncrasies.

Why does overfitting occur later? As with when we talked about local minimum, as the weights tend to
diverge from being close to zero the more complex the gradient surface becomes. Given enough iterations
the network will fit the noise and will not do a good job at representing the data.

To combat this problem one method is to implement \textbf{weight decay}. Weight decay is simply decreasing each
weight by some small factor on each iteration. This is equivalent to adding a penalty to large weights in
the error calculation. \textbf{This biases learning towards small weights, ie smooth surfaces, ie non-complex
decision surfaces}. A way to do this in the error calculation is:





$$E(w) \equiv \frac{1}{2} \displaystyle\sum\limits_{d} \displaystyle\sum\limits_{k\in outputs}(t_{kd} - o{kd})^2 + \gamma
\displaystyle\sum\limits_{i,j}w_{ji}^2$$

How many iterations should the algorithm perform? It should be enough to produce the lowest error over
the validation set. This can be done by \textbf{cross validation}: keeping two copies of the network. One which has the weights of
the best on the validation set so far. The other copy is the current training copy. Once the error
on the training copy is significantly lower than the validation set we can terminate the algorithm
and output the best network so far with respect to the validation set.

But what about small data sets where we cannot split the data up into training and validation sets? We can
use the \textbf{k-fold cross validation} approach. This is done by splitting up the data into k sets that
partition the data. You can then run the procedure k times each time using a different set as the validation
set. Each time we count the number of iterations that produced the best on the validation set. Average the 
count and then run BP on the WHOLE set that number of times.

\underline{Alternative Error minimization procedures:}
When minimizing error we make two decisions: choosing the direction to move and how far to move. Another
method is \textbf{line search}: once a line is chosen that specifies the direction, choose a distance by finding
the minimum of the error along this line. Another that builds off this is the \textbf{conjugate gradient} method
this is done by multiple line searches and I don't know.


\underline{Recurrent networks:}
If we would like the network at time t to depend on time t-1 we can add a hidden unit (b) and a input unit (c). Unit b
takes all the inputs, including unit c as inputs. It's output is then c at time t+1. This allows b to summarize earlier
values of the inputs that are arbitrarily far away in time. You can add several layers between the inputs and b. You could
also add several more b and c units in parallel.

How do we train? You can ``unfold'' the network and use BP directly. There are also better ways without unfolding that are
not included in the book.

\underline{Dynamically Modifying Network Structure:}

Adding units: Add a single fixed unit at a time. Calculate the residual error for each training example. Create a psuedo
network with one unit and all the inputs to it. For each training example make the target output to be the residual
error of that training example. Then plop that new unit in the network and keep it's weights fixed.

Removing units:  

\end{question}


\begin{question}{Computational Learning Theory:}
How can we characterize machine learning algorithms? What paradigms can we define in order to answer questions such as, ``how many training examples do we need in order to successfully learn a target concept?'' And, ``what do we define as successful learning?'' The fundamental purpose of computational learning theory is to provide frameworks for answering such meta-questions about machine learning algorithms. We will focus on two such frameworks: the probably approximately correct (PAC) learning framework and the mistake bounds framework.
\\
We are primarily concerned with characterizing learning algorithms in these two dimensions: (1) sample complexity and (2) computational complexity. Sample complexity refers to the number of training examples needed to successfully learn a target concept. Computational complexity refers to the amount of computational power required to successfully learn a target concept. Note that both of these dimensions must be polynomial.
\\
\\
\underline{True Error} The true error of a hypothesis $h$ with respect to target concept $c$ and true distribution $D^*$ is the probability that $h$ will misclassify an instance drawn uniformly at random from $D^*$ $$error_{D^*} = Pr_{x\in D^*}[c(x) \not = h(x)]$$
\\
\\
\textbf{Definition} A learner $L$ is \textbf{consistent} if $L$ outputs a hypothesis that perfectly fits the training data if the training data is noise-free. Note that perfectly fitting should not be confused with zero training error as zero training error most likely indicates overfitting! In addition, one constraint for a consistent leaner is that the hypothesis space must be guaranteed to include the target concept (that is, $C \subset H$).
\\
\\
\textbf{Definition} A learner $L$ is an \textbf{agnostic} learner if we are not guaranteed that the target concept is inside our hypothesis space. In this case, $L$ need only output the hypothesis in $H$ that has the minimum error over the training examples, defined as $error_D(h)$, where $D$ is the distribution of the training data.
\\
\\
\underline{PAC Learnability} The chief purpose of the PAC learning framework is to define the class of target concepts that can be successfully learned in polynomial time and with a polynomial amount of training examples.
\\
\\
\textbf{Definition:} Let $C$ be a concept class defined over a set of instances $X$, where $|X| = n$. Let $L$ be some learner using hypothesis space $H$. The concept class $C$ is \textbf{PAC-Learnable} by $L$ if for all concepts $c \in C$, distributions $D^*$ over $X$, $0 < \epsilon < \frac{1}{2}$, and $0 < \delta < \frac{1}{2}$, $L$ will output a hypothesis $h$ such that $$P(error_{D^*}(h) \leq \epsilon) = (1 - \delta)$$ in time that is polynomial in $\frac{1}{\epsilon}$, $\frac{1}{\delta}$, $n$, and the number of features necessary to describe a concept $c$.
\\
\\
\underline{Sample Complexity}
\\
The growth in the number of training examples required to learn a particular problem--defined as the sample complexity of the learning problem--is an important metric for determining whether or not a concept class is PAC-Learnable. 
\\
\textbf{Sample Complexity for Finite Hypothesis Spaces}
\\
\\
For a \textbf{consistent learner} with finite hypothesis space $H$, the number of training examples $m$ required to PAC-Learn the target concept class, assuming that $C \subset H$, is $$m \leq \frac{1}{\epsilon}(\ln |H| + \ln \frac{1}{\delta})$$ It is worthy to note that $m$ grows linearly with respect to $\frac{1}{\epsilon}$ and logarithmically in $\frac{1}{\delta}$ and with respect to the size of the hypothesis space.
\\
\\
If $L$ is an \textbf{agnostic learner}, then the we note that we must guarantee that $$P[error_{D^*}(h) > error_D(h) + \epsilon] \leq 2e^{-2 m \epsilon^2}$$ since we can no longer guarantee that $C \subset H$. Moreover, to ensure that the best hypothesis found by $L$ has an error bounded as above, then we must consider every $h \in H$ as having this error. Thus, we define: $$\delta = P[(\exists h \in H)(error_{D^*}(h) > error_D(h) + \epsilon)] \leq |H|2e^{-2 m \epsilon^2}$$Therefore, the number of training examples $m$ is:$$m \geq \frac{1}{2\epsilon^2}(\ln |H| + \ln \frac{1}{\delta})$$
\\
\textbf{Sample Complexity for Infinite Hypothesis Spaces}
\\
In the case where $|H| = \infty$, we must define a new measure of the expressivity  of the hypothesis space, which will directly influence the number of training examples necessary to learn a target concept successfully. We will use the VC-Dimension of a hypothesis space as this metric.
\\
\\
\textbf{Definition:} A \textbf{dichotomy} for a hypothesis $h$ and finite set of instances $S$ is a partition of $S$ into distinct subsets, where all instances in each subject are labeled by the hypothesis as belonging to a particular class. In the Boolean case, there are $2^|S|$ dichotomies, and in each dichotomy, all instances are partitioned into one of two sets: positive $\{x \in S | h(x) = 1\}$ and negative $\{ x \ in X | h(x) = 0 \}$
\\
\\
\textbf{Definition:} A set of instances $S \subset X$ is \textbf{shattered} by hypothesis space $H$ if any only if for every dichotomy of $S$, $\exists h \in H$ such that $h$ is consistent with the dichotomy.
\\
\\
\textbf{Definition:} The \textbf{Vapnik-Chervonenkis dimension}, $VC(H)$, of a hypothesis space $H$ defined over a space of instances $X$ is the size of the largest finite subset of $X$ that is shattered by $H$. If an arbitrarily large finite set of $X$ can be shattered by $H$, then $VC(H) = \infty$. Note that for any finite hypothesis space, $VC(H) \leq \log_2 |H|$ since there must be at least $2^d$ distinct hypothesis in $H$ if $VC(H) = d$. 
\\
\\
As described in ``Machine Learning'' by Mitchell (p217), the minimum number of training examples $m$ required to sufficiently learn a target concept $C$ is $$m \geq \frac{1}{\epsilon}(4 \log_2 \frac{2}{\delta} + 8 VC(H) \log_2(\frac{13}{\epsilon}))$$
\\
\\
Assume that for some concept class $C$, $VC(C) \geq 2$, and that we have a learner $L$, where $0 < \epsilon < \frac{1}{8}$ and $0 < \delta < \frac{1}{100}$. Then, $\exists $ distribution $D^*$ and target concept $c \ in C$ such that if $L$ observes fewer than $max [\frac{1}{\epsilon} \log_2 \frac{1}{\delta}, \frac{VC(C) - 1}{32\epsilon}]$ training examples then $L$ will output a hypothesis $h$ having $error_{D^*}(h) > \epsilon$ with probability at least $\delta$.
\\
\\
\\
The \textbf{VC-Dimension for directed acyclic layered networks} of perceptrons, $N_P$, is $$VC(N_P) \leq 2(r+1)s \log_2(es)$$, where $s$ is the number of internal nodes (including all output nodes), $r$ is the maximum input into any single perceptron, and $e$ is Euler's number.
\\
\\
\underline{Weighted-Majority algorithm and Mistake Bounds} \\
The weighted majority learning algorithm makes predictions by taking a weighted vote among a pool of prediction algorithms and outputs the most popular vote. The algorithms learns by altering the weight associated with each prediction algorithm. Whenever a prediction algorithm misclassifies a new training example, its weight is decreased by multiplying it by some real $\beta$, where $0 \leq \beta < 1$. Note if $\beta = 0$ then weighted majority is equivalent to the halving algorithm.
\\
\\
The \textbf{relative upper-mistake-bound for Weighted Majority} is $$2.4(k + \log_2 n)$$ where $k$ is the minimum number of mistakes made by algorithm $A$, which contains $n$ prediction algorithms, when training on a sequence of instances $D$. In addition, it is assumed that $\beta = \frac{1}{2}$ .
\\
\\
For an arbitrary $\beta$, the upper bound on the number of mistakes necessary to learn some target concept $c$ using a weighted majority algorithm with $n$ prediction algorithms is $$\frac{k \log_2 \frac{1}{\beta} + \log_2 n}{\log_2 \frac{2}{1+\beta}}$$
\end{question}

\begin{question}{Evaluating and Comparing Hypotheses}
	When evaluating a hypothesis, we are interested in answering these three questions:
	\begin{itemize}
		\item Given the observed accuracy of a hypothesis over a limited sample of data, how well does this accuracy generalize over future, unobserved data?
		\item Given that one hypothesis outperforms another over some set of data, how probable is it that this hypothesis is more accurate in general?
		\item What is the best way to use limited data to both learn a hypothesis and estimate its accuracy? When we attempt to do this, we must be observant of the hypothesis' bias (potential overfit) of the limited data and the variance inherit in the hypothesis' estimate.
	\end{itemize}
	The \textbf{sample error} of a hypothesis $h$ with respect to a target function $f$ and data sample $S$ is $$error_s(h) = \frac{1}{n}\sum_{x \in S}{\delta(f(x), h(x))}$$ where $|S| = n$ and $\delta(f(x), h(x))$ is one iff $f(x) \not = h(x)$ and zero otherwise (it's an indicator function for error in the hypothesis).
	\\
	\\
	The \textbf{true error} of a hypothesis $h$ respect to a target function $f$ and true data distribution $\mathcal{D}$ is exactly the probability that $h$ will misclassify an instance drawn at random according to the distribution $\mathcal{D}$, denoted $P_{x \in \mathcal{D}}$ $$error_{\mathcal{D}} = P_{x \in \mathcal{D}}(f(x) \not = h(x))$$
	\\
	\\
	We use confidence intervals to quantify the extent that our sample error estimates the true error of a hypothesis. In order to use a confidence interval, we must have the following three conditions:
	\begin{itemize}
		\item The sample $S$ contains $n$ examples drawn independent of one another, and independent of $h$, according to the probability distribution $\mathcal{D}$.
		\item We must have at least 30 examples $(n \geq 30)$.
		\item Hypothesis $h$ commits $r$ errors over the $n$ examples.
	\end{itemize}
	
	With these and only these three conditions, statistical theory states that the most probable value of the true error of the hypothesis is the sample error of the hypothesis. In addition, with $N\%$ probability, the true error lies in the interval $$error_S(h) \pm z_N \sqrt{\frac{error_S(h)(1-error_S(h))}{n}}$$
	
	Note that this \textbf{two-sided confidence interval} that bounds the true error of the hypothesis $h$ (with a given probability) assumes that the sample is drawn at random according to the true probability distribution and that the data is independent of the hypothesis being tested.
	\\
	\\
	A \textbf{one-sided confidence interval} is used to bound the true error of a hypothesis from either above or below. In the first case, we give a probability that the true error is less than or equal to a certain bound and in the latter case we give a probability that the true error is greater than or equal to a boundary. Using a one-sided confidence interval is useful for answering the a question such as, ``what is the probability that hypothesis $h$ has error less than U?''
	\\
	For Normally distributed random variables, there is a very easy way to calculate a one-sided confidence interval. Since the Normal distribution is symmetric about its mean, any two-sided confidence interval can be converted to a corresponding one-sided interval with twice the confidence. Thus, a $100(1-\alpha)\%$ two-sided confidence interval ($L$,$U$) implies a $100(1-\alpha/2)\%$ confidence interval with either a lower bound $L$ or an upper bound $U$ (note that $\alpha$ is the probability that the correct value lies outside the stated interval).
	\\
	\\
	Often, we are interested in the \textbf{difference in error between two hypotheses}. We define this true difference as $$d = error_{\mathcal{D}}(h_1) - error_{\mathcal{D}}(h_2)$$. We estimate this difference with sample error as $$\hat d = error_S(h_1) - error_S(h_2)$$. Note that $\hat d$ gives an unbiased estimate of $d$ ($E[\hat d] = d$), assuming that the samples were drawn using the same method (bias).
	\\
	If the error is Normally distributed, which it often is, then we see that the difference of two Normally distributed random variables is also Normally distributed. Thus, we can apply all of our previous knowledge about one and two sided confidence intervals to the difference in the error of two hypotheses. Thus, we can express confidence intervals on $\hat d$ or $d$
	$$\hat d \pm z_N \sqrt{\frac{error_{S_1}(h_1)(1-error_{S_1}(h_1))}{n_1} + \frac{error_{S_2}(h_2)(1-error_{S_2}(h_2))}{n_2}}$$
	Where the variance is
	$$\sigma_{\hat d}^2 = \frac{error_{S_1}(h_1)(1-error_{S_1}(h_1))}{n_1} + \frac{error_{S_2}(h_2)(1-error_{S_2}(h_2))}{n_2}$$
	This is for a two-sided confidence interval. It follows that we can make an appropriate one-sided confidence interval as well.
	\\
	\\
	Sometimes, we are interested in the probability that some conjecture is true, such as ``the true error of hypothesis one is less than the true error of hypothesis two.'' This is known as \textbf{hypothesis testing}. 
	
	\textbf{Common Distributions}:
	\begin{itemize}
		\item The \textbf{binomial distribution} characterizes the probability of observing $r$ heads in a series of $n$ independent coin tosses where the probability of heads in each toss is $p$ (note that $r, p, n$ are the parameters of a binomial distribution).
		For any value of $r \in [n] \cup \emptyset$, the binomial distribution dictates $$P(r) = \frac{n!}{r!(n-r)!}p^r(1-p)^{n-r}$$
		The mean is $np$ and the variance is $np(1-p)$, with standard deviation $\sqrt{np(1-p)}$. It is worthy to note that, for very large values of $n$, the binomial distribution is closely approximated by a Normal distribution. 
		\item The \textbf{normal distribution} is a bell-shaped distribution that is defined by the probability density function (pdf) $$p_n(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}^{e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}}$$ Note that the mean $\mu$ and standard deviation $\sigma$ are the only two parameters of a normal distribution.
		\\The probability of any interval $[a,b]$ in the Normal distribution is $\int_{a}^{b}{p_n(x)\textit{d}x}$
		\\The mean of the Normal distribution is $\mu$
		\\The variance of the Normal distribution is $\sigma^2$. The standard deviation is $\sigma$
		\\The \textbf{Central Limit Theorem} states that the sum of a large number of independent, identically distributed random variables follows a distribution that is approximately Normal.
	\end{itemize}
	
	\textbf{Statistical Terms} See Mitchell p133 for a more comprehensive list.
	\begin{itemize}
		\item An \textbf{estimator} is a random variable $Y$ that is used to estimate some parameter $p$ of an underlying population.
		\item The \textbf{estimation bias} of $Y$ as an estimator for $p$ is the quantity $(E[Y] - p)$. An unbiased estimator as a zero estimation bias.
		\item An $N\%$ \textbf{confidence interval} for parameter $p$ is an interval that includes $p$ with probability $N\%$.
		\item The \textbf{expected value, or mean} of a random variable $Y$ is $E[Y] = \sum_{i}{y_i * P(Y = y_i)} = \mu_Y$
		\item The \textbf{variance} of a random variable $Y$ is $Var(Y) = E[(Y - \mu_Y)^2]$. The variance characterizes the width of the distribution about its mean.
		\item The \textbf{standard deviation} of a random variable $Y$ is defined as the square root of the variance of $Y$. 
	\end{itemize}
\end{question}


\begin{question}{Bayesian Learning and the Naive Bayes Classifier}
	Bayesian inference gives us a probabilistic framework for inference. We assume that the quantities of interest are governed by probability distributions, and reasoning about these distributions with observed data can yield us optimal decisions.
	\\
	\\
	At the core of Bayesian learning methods is \textbf{Bayes' Theorem}, which allows us to calculate the probability of a hypothesis based upon the prior probability of the hypothesis, the probability of observing the training data given that the hypothesis is true, and the probability of observing the training data. We define the prior as $P(h)$, the likelihood of observing the data given the hypothesis as $P(D|h)$, and the probability of observing the data as $P(D)$. With these three quantities, we can calculate the posterior probability of the hypothesis given the data, $P(h|D)$, as 
	$$P(h|D) = \frac{P(h)P(D|h)}{P(D)}$$
	\\
	\\
	From Bayes' theorem, we can find the maximal posterior hypothesis, or the \textbf{maximum a posteriori (MAP)} hypothesis. 
	$$h_{MAP} = \argmax_{h \in H} P(h|D) = \argmax_{h \in H} P(D|h) P(h)$$
		We say that a learning algorithm is a \textbf{consistent learner} iff the learner outputs a hypothesis that commits zero errors over the training examples. Moreover, every consistent learner outputs a MAP hypothesis if we assume that the prior on each hypothesis is equal and if we assume noise-free, deterministic training data.
	\\
	\\
	If we assume that every hypothesis in $H$ has equal probability--the prior on each hypothesis is equal, then the MAP hypothesis is known as the \textbf{maximum likelihood (ML)} hypothesis. 
	$$h_{ML} = \argmax_{h \in H} P(D|h)$$
	
	In the case where we assume that that probability distribution underlying the data has Normally distributed noise (with zero mean), we have a different derivation for the ML hypothesis:
	$$h_{ML} = \argmin_{h \in H} \sum_{i=1}^{m}{(d(x_i) - h(x_i))^2}$$
	Where $d(x_i)$ is the value of the $i^{\text{th}}$ training example out of $m$ training examples and $h(x_i)$ is the hypothesis' label for this example.
	\\
	\\
	In light of Occam's Razor, we are motivated to find the simplest hypothesis to explain the data. The \textbf{Minimum Description Length} principle is the MAP hypothesis interpreted in the context of information theory:
	\begin{eqnarray}
		h_{MAP} &=& \argmax_{h \in H} P(D|h)P(h)\\
						&=& \argmax_{h \in H} \log_2 P(D|h) + \log_2 P(h)\\
						&=& \argmin_{h \in H} -\log_2 P(D|h) - \log_2 P(h)
	\end{eqnarray}
	We see that $-\log_2 P(h)$ is the number of bits necessary to encode $h$ under the optimal encoding of the hypothesis space $H$. We use the notation $L_{C_H}(h)$. We also see that $-\log_2 P(D|h)$ is the minimum description length of the training data given the hypothesis $h$. We denote this quantity as $L_{C_{D|h}}(D|h)$. Thus, the \textbf{minimum description length hypothesis} is: 
	$$h_{MDL} = \argmin_{h \in H} L_{C_1}(h) + L_{C_2}(D|h)$$
	Where $C_1, C_2$ are the encoding schemas for the hypothesis space and data, given the hypothesis, respectively. If we let $C_1$ to be $C_H$ and $C_2$ to be $C_{D|h}$, then we see that $h_{MDL} = h_{MAP}$.
	\\
	\\
	The \textbf{Bayes optimal classification} is the most probable classification of an unobserved instance, given the training data. The most probable classification is calculated by combining the predictions of all hypotheses ($P(v_j|h_i)$), each weighted by their respective posterior probabilities ($P(h_i|D)$).
	$$v_{optimal} = \argmax_{v_j \in V} \sum_{h_i \in H}{P(v_j|h_i)P(h_i|D)}$$
	\\
	\\
	The \textbf{Gibbs algorithm} is very simple: choose a hypothesis according to the probability distribution governing the hypothesis space and then use this hypothesis to predict the value of the next unobserved instance. Repeat for each new instance. Although this algorithm is extremely simple, it has been shown that, if each hypothesis is equally likely, then the expected error of the Gibbs algorithm is at most twice that of the Bayes' optimal classifier.
	\\
	\\
	One serious problem with optimal Bayes learning, however, is that we need exponential time in order to compute \textit{one} classification. Each hypothesis has $n$ attributes that are used to describe the hypothesis. Thus $P(v_j|h) = P(v_j|a_1, a_2, \ldots, a_n)$. In the case where each attribute is boolean, we have that our hypothesis space is $2^n$. Thus, maximizing over the hypothesis space will take exponential time.
	\\
	\\
	In order to ameliorate our exponential time issues with optimal Bayes learning, we can assume that the attribute values are conditionally independent given the target value. That is, $P(a_1, \ldots, a_n | v_j) = \prod_{i=1}^{n}{P(a_i|v_j)}$. Given this assumption, we have the \textbf{Naive Bayes Classifier}:
	$$v_{NB} = \argmax_{v_j \in V} P(v_j) \prod_{i}^{n}{P(a_i|v_j)}$$
	When we use a Naive Bayes Classifier in practice, we must estimate each prior and likelihood term. Often, the best way to do this is to use the relative frequencies of each term as provided by the labeled training data. Thus, $P(v_j)$ is the frequency of instances that have label $v_j$ to all instances, and any $P(a_i|v_j)$ term is the frequency of the instances that have attribute value $a_i$ to all instances that have label $v_j$. 
	\textbf{Probability Formulas}
	\begin{itemize}
		\item \textit{Independence} If events $A,B$ are independent then $P(A|B) = P(A)$ and $P(B|A) = P(B)$. Note that mutually exclusive events are, by definition, dependent upon one another.
		\item \textit{Product Rule}: $P(A \wedge B) = P(B)P(A|B) = P(A)P(B|A)$
		\item \textit{Sum Rule}: $P(A \vee B) = P(A) + P(B) - P(A \wedge B)$
		\item \textit{Marginalization}: Let events $A_1, \ldots, A_n$ partition the universal set. Then we have, for any event $B$, $P(B) = \sum_{i=1}^{n}{P(A_i)P(B|A_i)}$
	\end{itemize}
\end{question}

\begin{question}{Bayes Networks and Expectation Maximization}
	


\textbf{The EM Algorithm}

The EM algorithm can be applied in manny settings where we 
wish to estimate some set of parameters $\Theta$
that describe an underlying probability distribution, given only the observed portion of
the fill data produced by this distribution.

The observables are of the form $X = \{x_1, ... x_m\}$ and the unobserved data are
in the form $Z = \{z_1, ... z_m\}$. We denote the full data as $Y = X \cup Z$

In general we do not know the underlying distribution of
$\Theta$, since it is what we are trying to estimate. To overcome
this the EM algorithm uses the current hypothesis as the model to estimate
the distribution governing Y.

We define the function $Q(h'|h)$ that gives $E[ln P(Y|h')]$ as a function
oh $h'$, under the assumption that $\Theta = h$
$$Q(h'|h) = E[\ln p(Y|h')|h, X]$$

We then repeat the following steps till convergence:

\textbf{Step 1:} \emph{Estimation Step}: Calculate $Q(h'|h)$ using the current hypothesis
    $h$ and the observed data X to estimate the probability distribution over $Y$
    $$Q(h'|h) \leftarrow E[\ln p(Y|h') | h, X]$$
 
\textbf{Step 2:} \emph{Maximization step}: Replace hypothesis $h$ by the hypothesis $h'$
that maximizes this Q function.
$$h \leftarrow \arg\max_{h'} Q(h'|h)$$

When Q is continuous the EM algorithm converges to a stationary
point of the likelihood function $P(Y|h')$. It follows the normal
problems that occur when you have multiple optima.

\end{question}




\begin{question}{Hidden Markov Models}

Elements of the model:

States: $S = \{ s_0, s_1,.. s_m \}$\\
Transition probabilities: $P(q_t = s_i | q_{t-1} = s_j) = a_{ji}$\\
Output probability distribution: $P(y_t = O_k | q_t = s_j) = b_{j}^(k)$

\textbf{HMM Problems and Solutions:}

\underline{Evaluation:}\\
We want to compute the probability of observation sequence given a model. We use the
\emph{forward algorithms} and \emph{Viterbi algorithm}

\underline{Decoding:}\\
We want to find the state sequence that is most probable given the observation sequence.
We use the \emph{Viterbi algorithm} to do this.

\underline{Training:}\\
We want to find the model params (transition probabilities and emission probabilities)
to maximize the probability of the observed sequences. The solution
is the \emph{Forward-Backward Algorithm}

\textbf{Forward Algorithm}\\
$\alpha_t(j) = P(O_1 O_2 ... O_t, q_t = s_j | \lambda)$

$\alpha_0(j) = \text{ 1 if j start state, 0 otherwise}$\\
$\alpha_t(j) = \bigg{[}\sum^N\alpha_{t-1}(i)a_{ij}\bigg{]}b_j(O_t)$

$P(O|\lambda) = \alpha_T(S_N)$\\
Computation is $O(N^2T)$

\textbf{Backward Algorithm}\\
    $\beta_t(i) = P(O_{t+1}O_{t+2}..O_T | q_t = s_i, \lambda)$

    $\beta_T(i) = \text{ 1 if i is end state 0 otherwise}$\\
    $\beta_t(i) = \sum_j^N a_{ij}b_j(O_{t+1})\beta_{t+1}(j)$\\

    $P(O|\lambda) = \beta_0(S_0) = \alpha_T(S_N)$ 

\textbf{Viterbi Algorithm}\\
    Find state sequence $Q$ wich maximizes $P(O, Q|\lambda)$

    $VP_t(i) = MAX_{q_0,..q_{t-1}} P(O_1O_2 ... O_t, q_t = i | \lambda)$\\
    $VP_t(j) = MAX_{i=0, ... N} VP_{t-1}(i) a_{ij}b_j(O_t)$

    $P(O, Q | \lambda) = VP_T(s_N)$

\textbf{Training HMM parameters}\\
Tune $\lambda$ to maximize $P(O|\lambda)$\\
No efficient algorithm for global optimum\\
Efficient iterative algorithm finds local optimum\\
The \emph{Baum-welch (forward-backward) re-estimation} Computes the probabilities
for $\lambda$, then refines the estimation based on computed values from forward backward
algorithm.

\textbf{Forward-Backward Algorithm}\\
$\xi(i,j) = $ the probability of transiting from $s_i$ to $s_j$ at time t given O\\
$\xi(i,j) = P(q_t = s_i, q_{t+1} = s_j| O, \lambda) = \frac{\alpha_t(i)a_{ij}b_j(O_{t+1})\beta_{t+1}(j)}{P(O|\lambda)}$ 

\textbf{Baum-Welch Reestimation}\\
$\hat{a}_{ij} = \frac{ \text{expected number of trans from $s_i$ to $s_j$}}{\text{expected number of trans from $s_i$}}
= \frac{\sum_{t=0}^{T-1} \xi_t(i,j)}{ \sum_{t=0}^{T-1}\sum_{j=0}^{N} \xi_t(i,j)}$

$\hat{b}_j(k) = \frac{\text{expected number of times in state j with symbol k}}{\text{expected number of times in state j}}
= \frac{\sum_{t:O_t=k} \sum_{i=0}^N \xi_t(i,j) }{\sum_{t=0}^{T-1}\sum_{i=0}^{N} \xi_t(i,j)}$

We know that $P(O| \lambda^{c+1}) \ge P(O|\lambda^{c})$



\end{question}





\begin{question}{Instance-Based Learning}

What sets local methods apart from what we've learned:

\begin{itemize}
    \item Generalizing is done at query time\\
    \item Each distinct query can produce a different approximation to the target function\\
    \item Advantageous when function as a whole is too complex to model, but by confining
            the generalization to local instances can produce a satisfactory result. The
            model as a whole then becomes a collection of local approximations.
\end{itemize}

\textbf{KNN}

Given a query look at k nearest training examples, and take a majority vote to classify the query.

This can be changed to approximate real valued functions by taking an average value of
the K nearest neighbors.

\underline{Distance weighted KNN}

Similair to KNN, but add a weight factor to each vote that is inversely proportional
to distance. For example:

$$w_i \equiv \frac{1}{d(x_q, x_i)^2}$$

$$ \hat{f}(x_q) \leftarrow \frac{\sum^kw_if(x_i)}{\sum^k{w_i}}$$

The normalization of the above function ensures that if all neighbors
have value $c$, then the new classification will be $c$ as well.

\underline{Remarks on KNN}

\textbf{Induction bias:} We assume that the classification of a query point
is most likely to be the classification of instances close in
Eucledian distance.

Given a large enough training set it is robust to noisy training data.
This is because a few incorrect votes here an there will not have
a significant effect on classfication.

A practical issue of KNN is that we need to calculate distance based on
all attributes of distance.

\textbf{Curse of Dimensionality:} Each attribute is represented in a dimension. If there
are many irrelevant (or even if there are a few) attributes then it can possibly
skew our distance measure. It is possible that two points may be close when 
considering irrelevent attributes, but much farther when only considering
relevent attributes. To fix this we can shorten the axis of irrelevant dimensions.
This can be done by learning a weight for each dimension. It may also be benificial
to completely remove some attributes.


\textbf{Kernel function:} function of distance that is used to determine the weight of each training example.
In other words, the kernel function is the function K such that $$w_i = K(d(x_i, x_q))$$

\textbf{Locally weight regression}

We can combine KNN and regression by:

\begin{itemize}
    \item Minimize squared error on the K nearest neighbors \\
    \item Minimize squared error on all examples, but use some kernel function to weight examples. \\
    \item Combine both, weight K nearest neighbors with a kernel function \\
\end{itemize}

In most cases the functon is approximated by a constant, linear, or quadratic function.
This is because the cost of approximating a more complex function for each instance
is too high, and simple approximations normally do a good job for a sufficiently
small subregion.

\textbf{Gaussian Kernel:}
$$K(x_a, x_b) \equiv e^{\frac{-D(x_a - x_b)}{2\sigma^2}}$$
We can apply a gaussian to R data points, add these gaussians
together, the divide this gaussian by n and we get a pdf for
predicting y given x\\
By changing $\sigma$ on the gaussians we can get kernel smoothing.


\textbf{Radial Basis Functions}

RBF netowrks are a type of ANN constructed from spatially localized
kernel function. These can be seen as a blend of instance-based
approaches (spatially localized influence of each kernel function) and
neural network approaches (a global approximation to the
target function is formed at training time rather than a local
approximation
at query time). RBF networks have been used successfully in applications such as interpreting
visual scenes, in which the assumption of spatially
local influences is well justified.

\textbf{Remarks on Lazy and eager learning:}

Lazy methods normally require low computation time during learning
but high computation at query time.
Lazy methods consider each query when deciding how to generalize beyond the
training examples, eager methods cannot do this. Before seeing the any queries
the eager methods choose a global generalization.

This means that lazy methods use a more expressive hypothesis space. 
Local methods can represent a global approximation by a collection of 
small approximations. 

\end{question}

\begin{question}{Genetic Algorithms}
\textbf{Good things about GA:}\\
Evolution is know to be successful\\
GAs can search spaces of hypothesis containing complex interacting
parts, where the impact of each part on overall hypothesis fitness may be difficult
to model \\
GA are easily parallelized\\

Elements of a GA:\\
    \textbf{Representation}: A way to represent a hypothesis (normally a bitstring)\\
    \textbf{Population}: a collection of hypothesis \\
    \textbf{Fitness function}: A function that evaluates the quality of a hypothesis\\
    \textbf{Crossover operation}: Given two parents, a crossover operation creates
                        two children. The crossover operation should only produce
                        valid children\\
    \textbf{Mutation}: The way in which hypothesis can mutate, and the rate at which
                            mutate.\\
    \textbf{Rate of replacement}: A rate that determines how much of the population
                            will be removed at each generation.

The representation is the hard bias

The initial population is the soft bias
\end{question}

\begin{question}{Reinforcement Learning}

We want to learn the \textbf{control policy} which is a function from
the state space to actions: $$ \pi : S \rightarrow A$$
which outputs an appropriate action $a \in A$ given a state $s \in S$

Reinforcement learning differs from other function approximations in:
\begin{itemize}
    \item \emph{Delayed Reward:} For normal functions we having training
        data of the form $(d, f(d))$, but in reinforcement learning this
        is not the case.  This introduces the problem of \emph{temporal credit
        assignment}: determining which of the actions in its sequence are
        to be credited with producing the eventual rewards.\\
    \item \emph{Exploration:} The learner faces a tradeoff in choosing
        whether to favor exploration of unknown
        states and actions (to gather new information), or exploitation of states
        and actions that it has already learned will yeild high reward.\\
    \item \emph{Partially observable states:} May not be able to completely determin the state\\
    \item \emph{Lifelong learning: Learning never ends} 
\end{itemize}

Cumulative value with respect to an arbitrary policy $\pi$:

$$ V^{\pi}(s_t) = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} ..$$

This equation is normally called the \emph{discounted cumulative reward}

Instead of the discounted cumulative reward we could use the \emph{finite horizon}
$\sum^h r_{t+i}$ or the \emph{average reward} $lim_{h\rightarrow \infty} \frac{1}{h} \sum^h r_{t+i}$

The task of learning is to find the \emph{optimal policy} $\pi^*$
$$ \pi^* \equiv \arg\max_{\pi}V^{\pi}(s), (\forall s)$$

We denote $V^{\pi^*}$ as $V^*$

\textbf{Q-Learning}

Each equation follows from the previous.
$$Q(s,a) \equiv r(s,a) + \gamma V^*(\delta(s,a))$$
$$ \pi^*(s) \equiv \arg\max_{a}Q(s,a)$$
$$V^*(s) = \max_{a'} Q(s, a')$$
$$\hat{Q}(s,a) \leftarrow r + \gamma \max_{a'}\hat{Q}(s', a')$$


\end{question}

\begin{question}{Bagging and Boosting}

Bagging  = \textbf{B}ootstrap \textbf{agg}regat\textbf{ing}

The general procedure of bagging is to first take the training data and divide it
into a training set and test set, and repeat this process some N times. We then train
a learner on all sets. The learner could be the same for all sets, or they can be different.
When trying to find the mapping for a new example, we run the example agaisnt all
N learners, and take a (weighted) majority vote.

With bagging the expected error for the aggredated hypothesis is lower
than the expected value for an individual hypothesis.


\textbf{When is the bagging useful}

Bagging is useful when the learner is \emph{unstable} (high variance), which means that a small
change to training set can cause a large change in output hypothesis. These are algorithms
like decision trees and neural networks. Although they are robust to noise, each training
example has a significant role in determining the hypothesis. 

Experimentally, bagging can help significantly for unstable learners, but can hurt
stable learners (such as KNN).

\textbf{Boosting}\\
-Learn a sequence of classifiers from a single training set.\\
-But reweighting examples to emphasize misclassified\\
-Final classifier = weighted combination of classifiers
\end{question}

The idea of boosting is to take multiple weak hypothesis and use a
weighted combination of them to form a new hypothesis. This is done incrementally.
We first start off with one hypothesis based from the data. For every 
instance it incorrectly classifies, we give those instances a higher weight,
and retrain. A new hypothesis. Each weak hypothesis is given a weight

It can be shown that if every weak hypothesis has accuracy more than $.5 + \gamma$,
   $\gamma > 0$, boosting can increase the performance of the weighted combination
   hypothesis infinitely.

\end{document}
