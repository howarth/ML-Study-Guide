\documentclass[11pt]{article}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{1pt}

\def\indented#1{\list{}{}\item[]}
\let\indented=\endlist


\newcounter{questionCounter}
\newcounter{partCounter}[questionCounter]
\newenvironment{question}[2][\arabic{questionCounter}]{%
    \addtocounter{questionCounter}{1}%
    \setcounter{partCounter}{0}%
    \vspace{.25in} \hrule \vspace{0.5em}%
        \noindent{\bf #1: #2}%
    \vspace{0.8em} \hrule \vspace{.10in}%
}{}
\renewenvironment{part}[1][\arabic{partCounter}]{%
    \addtocounter{partCounter}{1}%
    \vspace{.10in}%
    \begin{indented}%
       {\bf (#1)} %
}{\end{indented}}


%%%%%%%%%%%%%%%%% Identifying Information %%%%%%%%%%%%%%%%%
%% This is here, so that you can make your homework look %%
%% pretty when you compile it.                           %%
%%     DO NOT PUT YOUR NAME ANYWHERE ELSE!!!!            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\myname}{Daniel Howarth, Malcolm Greaves}
\newcommand{\assignment}{Final Review}
\newcommand{\course}{Final Review 10-601 Machine Learning S11}
\newcommand{\andrewid}{dhowarth, mwgreave}
\newcommand{\myandrew}{dhowarth@andrew.cmu.edu, mwgreave@andrew.cmu.edu}
\newcommand{\myhwnum}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancyplain}
\chead{\course}

\begin{document}
.
\\
\\
Created by Dan Howarth and Malcolm Greaves. Adapted significantly from "Machine Learning" by Tom M. Mitchell (1997).

\begin{question}{Overview of learning}
\textbf{Definition} A computer program is said to \textbf{learn} from experience $E$, with respect to some class of tasks $T$ and performance measure $P$, if the program's performance at tasks in $T$, as measured by $P$, improves with experience $E$. (Mitchell, p2)
\\
\\
Machine learning is about learning the \textbf{function}--the relationship between inputs and outputs--of some underlying target concept or phenomenon. 
\end{question}
\\
\\
Machine learning in the finite case is called \textbf{classification}, and machine learning in the infinite case is called \textbf{regression}.
\\
\\
The principle questions for all machine learning algorithms:
\\
\text{	}\text{	}\textbf{Representation Power (hard bias).}\\
\text{	}\text{	}\textbf{Preference bias (soft, rank, search).}\\
\text{	}\text{	}\textbf{Search feasibility/efficiency.}\\
\text{	}\text{	}\textbf{Generalization (Will perform well on new data?)}\\\



\begin{question}{Concept Learning: Sorry, the year 1967 and there is no other option.}
Note that all of the learning algorithms in this section exhibit a \textbf{hard bias}. That is, a restriction on the hypothesis space that is necessary to successfully learn the target concept.
\\
\\
\textbf{Definition:} Inferring a boolean-valued function from training examples of its input and output is learning a \textbf{target concept}. Note that this idea can be extended to finite classes and regression.
\\
\\
\textbf{The inductive learning hypothesis} is that any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples.
\\
\\
We are able to order hypothesis on the continuum from least specific to most specific. With this ordering, we are also able to define the set of hypothesis bounded by the most and least specific hypotheses.
\\
\\
\textbf{Definition} Let $h_i, h_j$ be hypotheses defined over some set of instances $X$. $h_J$ is \textbf{more general than or equal to} $h_i$ ($h_j \geq_g h_i$)if and only if $$(\forall x \in X)(h_i(x) = 1 \implies h_j(x) = 1)$$. And, in this case, hypothesis $h_i$ is \textbf{more specific than or equal to} $h_j$ (written $h_i \leq_s h_j$).
\\
\\
Moreover, we can define two simple algorithms--Find-S and Find-G--that are able to, respectively, find the most specific and most general hypotheses.
\\
FIND-S: (FIND-G with minimal change)*\\
\text{	} Let $h = $ most specific $h \in H$ \\
\text{	} for each $x \in X$ such that $c(x) = 1$: \\
\text{	} \text{	} \text{	} for each attribute $a_i$ in $h$:\\
\text{	} \text{	} \text{	} \text{	} \text{	} if $a_i$ is satisfied by $x$ then do nothing \\
\text{	}\text{	}\text{	}\text{	}\text{	} else replace $a_i$ in $h$ with the next more general constraint that is satisfied by $x$
\\
\\
Note, to make this FIND-G, (1) initialize $h$ to be the most general hypothesis in $H$ (2) iterate over $x$ that satisfy $c(x) = 0$ (3) if $a_i$ in $h$ disagrees with $a_i$ in $x$, then change the attribute $a_i$ in $h$ to the next most specific constraint that satisfies $x$.
\\
\\
\textbf{Definition} A hypothesis $h$ is \textbf{consistent} with a set of training examples $D$ if any only if $(\forall [x,c(x)] \in D)(h(x) = c(x))$. That is, $h$ classifies all instances correctly.
\\
\\
\textbf{Definition:} The \textbf{version space} of a hypothesis space $H$ with respect to a set of training examples $D$ (denoted $VS_{H.D}$) is the subset of hypothesis that are consistent with $D$. $$VS_{H,D} = \{ h \in H | Consistent(h,D) \}$$
\\
\\
We have two ways of representing the version space. We can either explicitly or implicitly represent the space. 
\\
\\
\text{	} \text{	} To explicitly represent $VS_{H,D}$, we can \text{list} every $h \ in H$, and \textbf{then}, for each training example, \text{eliminate} all hypothesis that are inconsistent with the example. The remaining hypotheses are precisely the hypotheses in $H$ consistent with $D$, and thus is a complete list of the version space.
\\
\\
\text{	} \text{	} To implicitly represent the version space, we can keep track of the most specific and most general hypothesis in $H$. These two hypotheses, $h_s$ and $h_g$, bound the set of consistent hypothesis, thus delimiting the version space. However, there may be more than one member of $H$ that is maximally general or specific. Thus, we formally define these two boundaries of the version space. The \textbf{candidate elimination} algorithm, which implicitly defines $VS_{H,D}$, is essentially running FIND-S and FIND-G in parallel, while keeping track of the general and specific boundaries (defined below).
\\
\\
\textbf{Definition} The \textbf{general boundary} $G$ of a hypothesis space $H$ and training data $D$ is the set of maximally general members of $H$ consistent with $D$ $$G = \{ g \in H | Consistent(g,D) \wedge (\not \exists h \in H)(h >_g g \wedge Consistent(h,D)) \}$$
\\
\\
\textbf{Definition} The \textbf{specific boundary} $S$ of a hypothesis space $H$ and training data $D$ is the set of maximally specific members of $H$ consistent with $D$ $$S = \{ s \in H | Consistent(s,D) \wedge (\not \exists h \in H)(h >_s s \wedge Consistent(h,D)) \}$$
\end{question}
\\
\\
It is worthy to note that \textbf{candidate elimination} will converge toward a hypothesis that correctly describes the target concept if there are no errors in the training examples and if there exists a hypothesis in $H$ that correctly describes the target concept. If either of these two conditions are not fulfilled, then candidate elimination will output the empty set as the version space (effectively learning that there is no hypothesis that captures the target concept).
\\
\\
One effective use of using candidate elimination to define a version space is to use the set of hypothesis output by candidate elimination in a voting scheme, where all hypotheses are "asked" to give the classification of an unobserved instance $x$, and the majority decision of all hypotheses is taken as the classification of $x$. Of course, non-equal weightings of hypotheses and different voting schemes may be used to construct a classifier in this manner.
\\
\\
\underline{The Importance of Bias in Learning}\\
Consider having an unbiased learner $U$. That is, the hypothesis space $H$ for $U$ is the power set of all instances $X$. $H$ has no bias because $H$ includes every possible classification of instances. However, since $H$ has no bias, $U$ will only be able to \textbf{memorize} the classification of instances it observes: $U$ has absolutely no ability to generalize beyond observed training instances. The reason why this is the case is because, after observing any instance $x$, exactly half of the hypotheses in $H$ will agree with the classification of $x$ and the other half will disagree with the classification of $x$.
\\
\\
One can think of unbiased learning as a problem of indecisiveness.
\\
\\
\textbf{Definition}: Let $L$ be a concept learning algorithm with hypothesis space $H$ for the set of instances $X$. Let $D_c = \{ <x,c(x)>\}$ be some set of training examples for concept $c$. $L(x, D_c)$ is the classification of instance $x$ by learner $L$ after training on data $D_c$. The \textbf{inductive bias} of $L$ is any minimal set of assertions $B$ that we assume to be true (to justify our inductive inferences as deductive inferences) such that for any target concept $c$ and corresponding training examples $D_c$ $$(\forall x \in X)[(B \wedge D_c \wedge x) \vdash L(x,D_c)]$$


\begin{question}{Information Theory}

\underline{Information:}
$I(E) = log_2 \frac{1}{P(E)}$

Think of information as a level uncertainty about an event. If a probability is very low then the information it gives is high.

Information is additive. The information in single coin flip is 1 bit. In k coin flips there is k bits of information.
Intuitively, adding logs is the same as multiplying their paramters.

\underline{Entropy:}

$H(S) = \displaystyle\sum\limits_i p_i I(s_i)$ 

Can take integral for continuous functions.

Properties:
\begin{itemize}
    \item Entropy is non-negative.
    \item Invariant with respect to order of inputs
    \item The further the distribution is from uniform, the lower the entropy.
\end{itemize}

\underline{Joint Entropy:}

$H(A,B) = \displaystyle\sum\limits_{a,b} P(A=a, B=b)\log\frac{1}{P(A=a, B=b)}$

$H(A,B) < H(A) + H(B)$ TODO: Is this only when dependent?

\underline{Conditional Entropy:}


$H(A/B) = \displaystyle\sum\limits_{b}P(B=b)H(A|B=b)$

We can ask "How much is B telling us about A" and we get...

\underline{Mutual Information}

";" means between

$I(X;Y) = H(X) - H(X|Y) = H(Y)-H(Y|X) = H(X) + H(Y) - H(X,Y) = H(X,Y) - H(X|Y) - H(Y|X)$
\\
$I(X;Y) = \displaystyle\sum\limits_{y \in Range(Y)}{\displaystyle\sum\limits_{x \in Range(X)}{p_{X,Y}(x,y) \log_2 \frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}}}$
\\
\\
$I(X;Y;Z) = I(X;Y) - I(X;Y/Z) = H(X,Y,Z) - H(X,Y) - H(X,Z) - H(Y,Z) + H(X) + H(Y) + H(Z)$
\\
\\
Note that $I(X;X) = H(X) - H(X|X) = H(X)$

Can be negative with three variables.

$I(X;Y) = 0$ and $I(X;Z) = 0$ does not imply $I(X;Y;Z) = 0$



\end{question}


\begin{question}{Decision Trees: Duuuuuude... we don't have to restrict the hypothesis space!}

The \textbf{inductive bias} of decision trees is a preference for small trees over larger trees.

Decision trees are at the core a \textbf{top-down, greedy search} through the space of possible decision trees.

Appropriate for:
\begin{itemize}
    \item Instances are rep by attribute-value pairs (as vectors)
    \item The target function has discrete output value pairs.
    \item Disjunctive descriptions may be required.
    \item The training data may contain errors
    \item The training data may contain missing attribute values
    \item Train attributes with different costs
    \item When you do not want to risk "throwing the baby out of the water"
\end{itemize}

\underline{Limitations:}
Pure form of ID3 performs no back tracking when searching. This is susceptible in the same way
that all hill climb searches without back tracking to converge to a locally optimal solutions that are not
globally optimal

\underline{Capabilities:}

ID3 uses all training examples at each step to perform a statistical analysis of how to
refine it's current hypothesis. An advantage of this is that the resulting search is less
sensitive to errors and noise in individual data. 

\textbf{Representation Power (hard bias):}

The hypothesis space of all decision trees is a \emph{complete} space of finite discrete-valued
functions, relative to the available attributes. 

It only keeps track a single current hypothesis when searching the space of DTs. Cannot keep track 
of all consistent hypotheses.

\textbf{Preference bias (soft, rank, search):}

The \textbf{inductive bias} of decision trees is a preference for small trees over larger trees. Trees
that place high information gain attributes near the root are preferred over those that do not.

\underline{Occam's razor:} Prefer the simplest hypothesis that fits that data.

\textbf{Search feasibility/efficiency:}

\textbf{Generalization (Will perform well on new data?)}


At each level the tree splits on the attribute that provides the max information gain.

$S_v = \{s\in S | A(s) = v\}$ is the subset of $S$ for which the attribute $A$ has value $v$.

$Gain(S,A) = Entropy(S) - \displaystyle\sum\limits_{v \in Values(A)} \frac{|S_v|}{|S|} Entropy(S_v)$

\underline{Definition:} Given a hypothesis space $H$, a hypothesis $h \in H$ is said to \textbf{overfit}
the data if $\exists h\prime \in H$, $h \ne h\prime$, such that $h$ has smaller error over the
training examples but $h\prime$ has smaller error over the entire distribtion of instances.

A DT can overfit the data even if the data is noise free. It is possible for \underline{coincidental regularities} to occur
in which some attribute happens ot partition the examples very well, despite being unrealted to the actual target
function. This happens more frequently when there are a small number of examples. 

To avoid over fitting you can do (1) stop growing the tree earlier, before it reaches the point where it perfectly
classifies the training data or (2) allow the tree to overfit the data, but then post-prune the tree.

The latter has been found to perform better in practice.

Ways to determine the correct final tree size: (1) Use a separate set of examples, distinct from the training
examples, to evaluate the utility of post-pruning nodes from the tree. (2) Use all the available data for training,
but apply statistical analasys to estimate whether exapnding (or pruning) a particular node is likely
to produce a better hypothesis

\underline{Reduced error pruning:}

Pruning a node consists of first removing the subtree at that node and assigning it the most common
classification of the training examples affiliated with that node. Those nodes that are removed are
those that do no worse on the pruning set than with the original.

Nodes are pruned iteratively, always choosing the node whose removal most increases the accuracy over the
validation set. Pruning is continued until harmful.

Pruning can be done top-down or bottom-up.

A drawback of pruning is that you have to give up some training data to be part of the validation set.

\underline{Rule post pruning:}

Each node in a DT represents a conjunction. Rule post pruning takes each conjunction and prunes any preconditions
whose removal improves accuracy. After this is done for each rule, sort the rules by their estimated accuracy.

The advantages:
\begin{itemize}
    \item Can consider to prune a sinle rule instead of a subtree.
    \item In decision trees when we consider removing nodes near the root it requires messy bookkeeping issues
    such as how to restructure the tree after removing the node while also retaining the structure of the 
    subtree below this test.
    \item Converting to rules improves human readability/understandability.
\end{itemize}

We can incorporate \textbf{continuous valued functions} be creating classifications such as whether or not
the number is below or above a certain number. These number too are choosed from their information gain.

\underline{Alternative measure for selecting attributes:}
There is a natural bias in the information gain measure that favors attributes with many values over
those with few values. Consider if Date was an attribute. It would pretty much always provide the most information.
This is because since there are so many (365) possible values it is much more probable for the attribute
to be able perfectly predict the target attribute over the training data.

A way to avoid this is to change the gain measure. If we incorporate split information (entropy):

$$SplitInformation(S,A) = - \displaystyle\sum\limits_{v\in values(A)}\frac{|S_v|}{|S|}\log_2\frac{|S_v|}{|S|}$$

This is the same thing the entropy of $S$ with respect to the values of attribute $A$. We can now use this
to give a penalty to those attributes that have uniform distributions:

$$GainRatio(S,A) \equiv \frac{Gain(S,A)}{SplitInformation(S,A)}$$


\underline{Handling training examples with missing attribute values:}

A simple method is to assign the missing attribute value to the value that is most common among
the training examples at that node. We could also assign it the value that is most common among
the training examples at that node with classification $c(x)$.

Another, more complex method is to assign a probability to each value rather than to the most common.
If $A(x) = 0$ with probability $.4$ and $A(x) = 1$ with probability $.6$ then assign the respective fractions
of the example to each value. These fractions are used for computing information gain.

\underline{Handling attributes with differing costs:}

This is useful when some attributes are costly the evaluate. For example when classifying symptoms we would not
want the first decision to be based off of an expensive lab test. To penalize for cost we can modify the gain
function:

$$\frac{Gain^2(S,A)}{Cost(A)}$$

or maybe possibly

$$\frac{2^{Gain(S,A)}-1}{(Cost(A) + 1)^w}$$

This introduces a \textbf{bias} towards lost cost attributes near the root of the tree. Which biases the search to 
low cost attributes.





\end{question}



\begin{question}{Artificial Neural Networks: }

\textbf{Appropriate for:}

\begin{itemize}
    \item Instance are represented by many attribute-value pairs (Can be rep as a vector) 
    \item The target value may be discrete valued, continuous, or a combination of multiple
    \item Training data may be noisy or contain errors
    \item Learning time isn't a concern
    \item Quick evaluation
    \item Understandability of underlying decision isn't a concern
    \item Multilayer networks are awesome for finding features in the data that a human would not pick up on. These
    features are implicit in the data.
\end{itemize}

\textbf{Representation Power (hard bias):}
Much is unknown about which classes of functions can be described with which types of networks.

\begin{itemize}
    \item Boolean function: Every boolean function can be represented with  two layers
    \item Continuous functions: Can be approximated with arbitrary low error with two layers
    \item Arbitrary functions: approximated with arbitrary accuracy with two layers
\end{itemize}

In all, we see that networks with small depths provide a very expressive hypothesis space.

\textbf{Preference bias (soft, rank, search):}

Search bias: Error gradient search.

Inductive bias: Smooth interpolation between data points. In words that make sense: If two training
examples are positive and have no negative examples between them then BP will classify instances
between as positive as well.

\textbf{Search feasibility/efficiency:}

\textbf{Generalization (Will perform well on new data?)}

\textbf{Perceptron training rule:}

Proven to converge to a consistent hypothesis given that the training examples are 
linearly separable, and a sufficiently small $\eta$ is used.

Update rule: $w_i \leftarrow w_i + \eta (t-o)x_i$

\textbf{Gradient Descent:}

Used when:
\begin{itemize}
    \item Hypothesis space contains continuously parameterized hypotheses
    \item The error can be differentiated with respect to the hypothesis parameters
\end{itemize}

Practical difficulties:
\begin{itemize}
    \item Can be pretty slow (can take thousands of iterations to converge)
    \item If there are multiple local minimum then it is not guaranteed to converge
\end{itemize}

If training data is not linearly separable it is proven to converge to best-fit
approximation.

In general: $\Delta w_i = - \eta \frac{\delta E}{\delta w_i}$


\underline{Standard GD:} 

Update rule: $w_i  \leftarrow w_i + \eta \sum_d (t_d - o_d)x_i$

Proven to converge because there is only one global minimum.

\underline{Stochastic GD update rule:}

Error defined as: $E_d = \frac{1}{2}(t_d - o_d)^2$

Update rule: $w_i \leftarrow w_i + \eta(t_d - o_d)x_i$

By taking $\eta$ sufficiently stochastic GD can be made to approximate the true
gradient descent arbitrarily closely.

Difference between Standard and Stochastic
\begin{itemize}
    \item Standard sums over all examples, stochastic only one example per time 
    \item Summing over all takes more computational time. However with standard we
    can use a larger $\eta$ (step size) because it calculates the true gradient.
    \item Stochastic is less prone to fall into local minimum because it uses various
    $E_d(w)$
\end{itemize}

\textbf{Backpropagation:}

\underline{English approximation of algorithm:}\\
For each training example apply it to the network:
\\ \text{	} \text{	} \text{	} Calculate the error, the difference in the label and the output of the network, for each of the output units. Then, backpropagate this error signal through the network, determining the error of each hidden unit (the partial derivative of the error with respect to each weight). Once we have calculated how much error each weight contributes to the total output, we will have our gradient: a vector of partial derivatives of the error with respect to each weight. We adjust all of the weights such that we "move" in the direction that minimizes the entire error of the network.
\\
Iterate this process until convergence (or until the error is acceptable).

\underline{Caveats:}\\
BP can have multiple local minimum. 

\underline{Sigmoid function}

$\sigma(y) = \frac{1}{1+e^-y}$

$\frac{\delta \sigma(y)}{\delta y} = \sigma(y)(1-\sigma(y)$

If we take the sigmoid function and multiply y by a constant k, we can vary
the steepness of the function.

$E(w) = \frac{1}{2}\displaystyle\sum\limits_{d} \displaystyle\sum\limits_{k\in outputs} (t_kd - o_kd)^2$

\underline{Update rules:}

Unit $j$: $w_i \rightarrow w_i + \eta \delta_j x_{ji}$

$\delta_j$ is the error term for unit $j$

Error term for output unit k: $\delta_k  = o_k(1 - o_k)(t_k - o_k)$

Error term for hidden unit j: $\delta_j =  o_j(1-o_j) \displaystyle\sum\limits_{l \in DS}\delta_lw_{lj}$\\
For hidden units think of it as the degree to which the unit is responsible for the error.

\underline{Momentum}

Update rule: $\Delta w_{ji}(n) = \eta \delta_j x_{ji} + \alpha \Delta w_{ji}(n-1)$

What's it do?
\begin{itemize}
    \item Is the gradient unchanging? Well then let's hurry this thing up.
    \item Keeps it moving in the same direction, which can keep the ball rolling through local minima
    \item If the gradient is flat then you wouldn't know where to go. Momentum keeps it rollin'
\end{itemize}

\underline{Convergence and god damn local minima:}

BP is not guaranteed to converge to local minima.

For high dimensional networks what may be a local min for one weight is probably not a local
min with respect to another weight. So as there are more weights, there are more "escape" routes
in which we can get out of local minimum. 

When the weights are initialized they are close to zero. At these points the sigmoid function
acts near linearly. This allows for a much smoother gradient surface when the algorithm starts out. 
When the weights are much different the surface is much more complex. These initial conditions
prove to be very useful because we hope that by the time the weights reach this point we hope
that the algorithm has already moved to a location near the global minimum, and local minimum
at this point are acceptable.

\underline{Heuristics to stay out of local min:}
\begin{itemize}
    \item Momentum
    \item Stochastic over Standard. Each training example has a different error gradient, and each one
    can have different local minimum, which makes it harder to get stuck in any one of them.
    \item Training multiple networks, but starting with different weights. This is basically just
    starting at different parts on the gradient surface. Choose best performer. Or you could use
    the different networks as voters for the output.
\end{itemize}


\underline{Hidden layer representations:}
Hidden layers learn/invent features that the human designer did not put into the algorithm. The higher
number of layers the higher the complexity of the features.

Remember the example of taking 8 binary strings and mapping them to themselves. This was able to be done
with a single hidden layer of 3 units. Un-surprisingly enough $2^3$ is eight.

\underline{Stopping Criterion, Overfitting, and generalization:}

One idea is to stop when error drops below a threshold. However this ends up being a poor stopping criterion
because it is very susceptible to overfitting.

Generalization Accuracy - Accuracy at which the network fits examples beyond the training data

Generalization accuracy get worse over while the training error decreases because the network
can be tuned to the noisy or idiosyncrasies of the training examples, and not tuned to the general
distribution of instances. The larger the number of weights the more degree of freedom the network
has to fit these idiosyncrasies.

Why does overfitting occur later? As with when we talked about local minimum, as the weights tend to
diverge from being close to zero the more complex the gradient surface becomes. Given enough iterations
the network will fit the noise and will not do a good job at representing the data.

To combat this problem one method is to implement \textbf{weight decay}. Weight decay is simply decreasing each
weight by some small factor on each iteration. This is equivalent to adding a penalty to large weights in
the error calculation. \textbf{This biases learning towards small weights, ie smooth surfaces, ie non-complex
decision surfaces}. A way to do this in the error calculation is:





$$E(w) \equiv \frac{1}{2} \displaystyle\sum\limits_{d} \displaystyle\sum\limits_{k\in outputs}(t_{kd} - o{kd})^2 + \gamma
\displaystyle\sum\limits_{i,j}w_{ji}^2$$

How many iterations should the algorithm perform? It should be enough to produce the lowest error over
the validation set. This can be done by \textbf{cross validation}: keeping two copies of the network. One which has the weights of
the best on the validation set so far. The other copy is the current training copy. Once the error
on the training copy is significantly lower than the validation set we can terminate the algorithm
and output the best network so far with respect to the validation set.

But what about small data sets where we cannot split the data up into training and validation sets? We can
use the \textbf{k-fold cross validation} approach. This is done by splitting up the data into k sets that
partition the data. You can then run the procedure k times each time using a different set as the validation
set. Each time we count the number of iterations that produced the best on the validation set. Average the 
count and then run BP on the WHOLE set that number of times.

\underline{Alternative Error minimization procedures:}
When minimizing error we make two decisions: choosing the direction to move and how far to move. Another
method is \textbf{line search}: once a line is chosen that specifies the direction, choose a distance by finding
the minimum of the error along this line. Another that builds off this is the \textbf{conjugate gradient} method
this is done by multiple line searches and I don't know.


\underline{Recurrent networks:}
If we would like the network at time t to depend on time t-1 we can add a hidden unit (b) and a input unit (c). Unit b
takes all the inputs, including unit c as inputs. It's output is then c at time t+1. This allows b to summarize earlier
values of the inputs that are arbitrarily far away in time. You can add several layers between the inputs and b. You could
also add several more b and c units in parallel.

How do we train? You can "unfold" the network and use BP directly. There are also better ways without unfolding that are
not included in the book.

\underline{Dynamically Modifying Network Structure:}

Adding units: Add a single fixed unit at a time. Calculate the residual error for each training example. Create a psuedo
network with one unit and all the inputs to it. For each training example make the target output to be the residual
error of that training example. Then plop that new unit in the network and keep it's weights fixed.

Removing units:  

\end{question}


\begin{question}{Computational Learning Theory:}
How can we characterize machine learning algorithms? What paradigms can we define in order to answer questions such as, "how many training examples do we need in order to successfully learn a target concept?" And, "what do we define as successful learning?" The fundamental purpose of computational learning theory is to provide frameworks for answering such meta-questions about machine learning algorithms. We will focus on two such frameworks: the probably approximately correct (PAC) learning framework and the mistake bounds framework.
\\
We are primarily concerned with characterizing learning algorithms in these two dimensions: (1) sample complexity and (2) computational complexity. Sample complexity refers to the number of training examples needed to successfully learn a target concept. Computational complexity refers to the amount of computational power required to successfully learn a target concept. Note that both of these dimensions must be polynomial.
\\
\\
\underline{True Error} The true error of a hypothesis $h$ with respect to target concept $c$ and true distribution $D^*$ is the probability that $h$ will misclassify an instance drawn uniformly at random from $D^*$ $$error_{D^*} = Pr_{x\in D^*}[c(x) \not = h(x)]$$
\\
\\
\textbf{Definition} A learner $L$ is \textbf{consistent} if $L$ outputs a hypothesis that perfectly fits the training data if the training data is noise-free. Note that perfectly fitting should not be confused with zero training error as zero training error most likely indicates overfitting! In addition, one constraint for a consistent leaner is that the hypothesis space must be guaranteed to include the target concept (that is, $C \subset H$).
\\
\\
\textbf{Definition} A learner $L$ is an \textbf{agnostic} learner if we are not guaranteed that the target concept is inside our hypothesis space. In this case, $L$ need only output the hypothesis in $H$ that has the minimum error over the training examples, defined as $error_D(h)$, where $D$ is the distribution of the training data.
\\
\\
\underline{PAC Learnability} The chief purpose of the PAC learning framework is to define the class of target concepts that can be successfully learned in polynomial time and with a polynomial amount of training examples.
\\
\\
\textbf{Definition:} Let $C$ be a concept class defined over a set of instances $X$, where $|X| = n$. Let $L$ be some learner using hypothesis space $H$. The concept class $C$ is \textbf{PAC-Learnable} by $L$ if for all concepts $c \in C$, distributions $D^*$ over $X$, $0 < \epsilon < \frac{1}{2}$, and $0 < \delta < \frac{1}{2}$, $L$ will output a hypothesis $h$ such that $$P(error_{D^*}(h) \leq \epsilon) = (1 - \delta)$$ in time that is polynomial in $\frac{1}{\epsilon}$, $\frac{1}{\delta}$, $n$, and the number of features necessary to describe a concept $c$.
\\
\\
\underline{Sample Complexity}
\\
The growth in the number of training examples required to learn a particular problem--defined as the sample complexity of the learning problem--is an important metric for determining whether or not a concept class is PAC-Learnable. 
\\
\textbf{Sample Complexity for Finite Hypothesis Spaces}
\\
\\
For a \textbf{consistent learner} with finite hypothesis space $H$, the number of training examples $m$ required to PAC-Learn the target concept class, assuming that $C \subset H$, is $$m \leq \frac{1}{\epsilon}(\ln |H| + \ln \frac{1}{\delta})$$ It is worthy to note that $m$ grows linearly with respect to $\frac{1}{\epsilon}$ and logarithmically in $\frac{1}{\delta}$ and with respect to the size of the hypothesis space.
\\
\\
If $L$ is an \textbf{agnostic learner}, then the we note that we must guarantee that $$P[error_{D^*}(h) > error_D(h) + \epsilon] \leq 2e^{-2 m \epsilon^2}$$ since we can no longer guarantee that $C \subset H$. Moreover, to ensure that the best hypothesis found by $L$ has an error bounded as above, then we must consider every $h \in H$ as having this error. Thus, we define: $$\delta = P[(\exists h \in H)(error_{D^*}(h) > error_D(h) + \epsilon)] \leq |H|2e^{-2 m \epsilon^2}$$Therefore, the number of training examples $m$ is:$$m \geq \frac{1}{2\epsilon^2}(\ln |H| + \ln \frac{1}{\delta})$$
\\
\textbf{Sample Complexity for Infinite Hypothesis Spaces}
\\
In the case where $|H| = \infty$, we must define a new measure of the expressivity  of the hypothesis space, which will directly influence the number of training examples necessary to learn a target concept successfully. We will use the VC-Dimension of a hypothesis space as this metric.
\\
\\
\textbf{Definition:} A \textbf{dichotomy} for a hypothesis $h$ and finite set of instances $S$ is a partition of $S$ into distinct subsets, where all instances in each subject are labeled by the hypothesis as belonging to a particular class. In the Boolean case, there are $2^|S|$ dichotomies, and in each dichotomy, all instances are partitioned into one of two sets: positive $\{x \in S | h(x) = 1\}$ and negative $\{ x \ in X | h(x) = 0 \}$
\\
\\
\textbf{Definition:} A set of instances $S \subset X$ is \textbf{shattered} by hypothesis space $H$ if any only if for every dichotomy of $S$, $\exists h \in H$ such that $h$ is consistent with the dichotomy.
\\
\\
\textbf{Definition:} The \textbf{Vapnik-Chervonenkis dimension}, $VC(H)$, of a hypothesis space $H$ defined over a space of instances $X$ is the size of the largest finite subset of $X$ that is shattered by $H$. If an arbitrarily large finite set of $X$ can be shattered by $H$, then $VC(H) = \infty$. Note that for any finite hypothesis space, $VC(H) \leq \log_2 |H|$ since there must be at least $2^d$ distinct hypothesis in $H$ if $VC(H) = d$. 
\\
\\
As described in "Machine Learning" by Mitchell (p217), the minimum number of training examples $m$ required to sufficiently learn a target concept $C$ is $$m \geq \frac{1}{\epsilon}(4 \log_2 \frac{2}{\delta} + 8 VC(H) \log_2(\frac{13}{\epsilon}))$$
\\
\\
Assume that for some concept class $C$, $VC(C) \geq 2$, and that we have a learner $L$, where $0 < \epsilon < \frac{1}{8}$ and $0 < \delta < \frac{1}{100}$. Then, $\exists $ distribution $D^*$ and target concept $c \ in C$ such that if $L$ observes fewer than $max [\frac{1}{\epsilon} \log_2 \frac{1}{\delta}, \frac{VC(C) - 1}{32\epsilon}]$ training examples then $L$ will output a hypothesis $h$ having $error_{D^*}(h) > \epsilon$ with probability at least $\delta$.
\\
\\
\\
The \textbf{VC-Dimension for directed acyclic layered networks} of perceptrons, $N_P$, is $$VC(N_P) \leq 2(r+1)s \log_2(es)$$, where $s$ is the number of internal nodes (including all output nodes), $r$ is the maximum input into any single perceptron, and $e$ is Euler's number.
\\
\\
\underline{Weighted-Majority algorithm and Mistake Bounds} \\
The weighted majority learning algorithm makes predictions by taking a weighted vote among a pool of prediction algorithms and outputs the most popular vote. The algorithms learns by altering the weight associated with each prediction algorithm. Whenever a prediction algorithm misclassifies a new training example, its weight is decreased by multiplying it by some real $\beta$, where $0 \leq \beta < 1$. Note if $\beta = 0$ then weighted majority is equivalent to the halving algorithm.
\\
\\
The \textbf{relative upper-mistake-bound for Weighted Majority} is $$2.4(k + \log_2 n)$$ where $k$ is the minimum number of mistakes made by algorithm $A$, which contains $n$ prediction algorithms, when training on a sequence of instances $D$. In addition, it is assumed that $\beta = \frac{1}{2}$ .
\\
\\
For an arbitrary $\beta$, the upper bound on the number of mistakes necessary to learn some target concept $c$ using a weighted majority algorithm with $n$ prediction algorithms is $$\frac{k \log_2 \frac{1}{\beta} + \log_2 n}{\log_2 \frac{2}{1+\beta}}$$
\end{question}




\begin{question}{Hidden Markov Models}

Elements of the model:

States: $S = \{ s_0, s_1,.. s_m \}$\\
Transition probabilites: $P(q_t = s_i | q_{t-1} = s_j) = a_{ji}$\\
Output probability distribution: $P(y_t = O_k | q_t = s_j) = b_{j}^(k)$

\textbf{HMM Problems and Solutions:}

\underline{Evaluation:}\\
We want to compute the probability of observation sequence given a model. We use the
\emph{forward algorithms} and \emph{Viterbi algorithm}

\underline{Decoding:}\\
We want to find the state sequence that is most probable given the observation sequence.
We use the \emph{Viterbi algorithm} to do this.

\underline{Training:}\\
We want to find the model params (transition probabilities and emission probabilities)
to maximize the probability of the observed sequences. The solution
is the \emph{Forward-Backward Algorithm}

\textbf{Forward Algorithm}\\
$\alpha_t(j) = P(O_1 O_2 ... O_t, q_t = s_j | \lambda)$

$\alpha_0(j) = \text{ 1 if j start state, 0 otherwise}$\\
$\alpha_t(j) = \bigg{[}\sum^N\alpha_{t-1}(i)a_{ij}\bigg{]}b_j(O_t)$

$P(O|\lambda) = \alpha_T(S_N)$\\
Computation is $O(N^2T)$

\textbf{Backward Algorithm}\\
    $\beta_t(i) = P(O_{t+1}O_{t+2}..O_T | q_t = s_i, \lambda)$

    $\beta_T(i) = \text{ 1 if i is end state 0 otherwise}$\\
    $\beta_t(i) = \sum_j^N a_{ij}b_j(O_{t+1})\beta_{t+1}(j)$\\

    $P(O|\lambda) = \beta_0(S_0) = \alpha_T(S_N)$ 

\textbf{Viterbi Algorithm}\\
    Find state sequence $Q$ wich maximizes $P(O, Q|\lambda)$

    $VP_t(i) = MAX_{q_0,..q_{t-1}} P(O_1O_2 ... O_t, q_t = i | \lambda)$\\
    $VP_t(j) = MAX_{i=0, ... N} VP_{t-1}(i) a_{ij}b_j(O_t)$

    $P(O, Q | \lambda) = VP_T(s_N)$

\textbf{Training HMM parameters}\\
Tune $\lambda$ to maximize $P(O|\lambda)$\\
No efficient algorithm for global optimum\\
Efficient iterative algorithm finds local optimum\\
The \emph{Baum-welch (forward-backward) re-estimation} Computes the probabilities
for $\lambda$, then refines the estimation based on computed values from forward backward
algorithm.

\textbf{Forward-Backward Algorithm}\\
$\xi(i,j) = $ the probability of transiting from $s_i$ to $s_j$ at time t given O\\
$\xi(i,j) = P(q_t = s_i, q_{t+1} = s_j| O, \lambda) = \frac{\alpha_t(i)a_{ij}b_j(O_{t+1})\beta_{t+1}(j)}{P(O|\lambda)}$ 

\textbf{Baum-Welch Reestimation}\\
$\hat{a}_{ij} = \frac{ \text{expected number of trans from $s_i$ to $s_j$}}{\text{expected number of trans from $s_i$}}
= \frac{\sum_{t=0}^{T-1} \xi_t(i,j)}{ \sum_{t=0}^{T-1}\sum_{j=0}^{N} \xi_t(i,j)}$

$\hat{b}_j(k) = \frac{\text{expected number of times in state j with symbol k}}{\text{expected number of times in state j}}
= \frac{\sum_{t:O_t=k} \sum_{i=0}^N \xi_t(i,j) }{\sum_{t=0}^{T-1}\sum_{i=0}^{N} \xi_t(i,j)}$

We know that $P(O| \lambda^{c+1}) \ge P(O|\lambda^{c})$



\end{question}





\begin{question}{Instance-Based Learning}

What sets local methods apart from what we've learned:

\begin{itemize}
    \item Generalizing is done at query time\\
    \item Each distinct query can produce a different approximation to the target function\\
    \item Advantageous when function as a whole is too complex to model, but by confining
            the generalization to local instances can produce a satisfactory result. The
            model as a whole then becomes a collection of local approximations.
\end{itemize}

\textbf{KNN}

Given a query look at k nearest training examples, and take a majority vote to classify the query.

This can be changed to approximate real valued functions by taking an average value of
the K nearest neighbors.

\underline{Distance weighted KNN}

Similair to KNN, but add a weight factor to each vote that is inversely proportional
to distance. For example:

$$w_i \equiv \frac{1}{d(x_q, x_i)^2}$$

$$ \hat{f}(x_q) \leftarrow \frac{\sum^kw_if(x_i)}{\sum^k{w_i}}$$

The normalization of the above function ensures that if all neighbors
have value $c$, then the new classification will be $c$ as well.

\underline{Remarks on KNN}

\textbf{Induction bias:} We assume that the classification of a query point
is most likely to be the classification of instances close in
Eucledian distance.

Given a large enough training set it is robust to noisy training data.
This is because a few incorrect votes here an there will not have
a significant effect on classfication.

A practical issue of KNN is that we need to calculate distance based on
all attributes of distance.

\textbf{Curse of Dimensionality:} Each attribute is represented in a dimension. If there
are many irrelevant (or even if there are a few) attributes then it can possibly
skew our distance measure. It is possible that two points may be close when 
considering irrelevent attributes, but much farther when only considering
relevent attributes. To fix this we can shorten the axis of irrelevant dimensions.
This can be done by learning a weight for each dimension. It may also be benificial
to completely remove some attributes.


\textbf{Kernal function:} function of distance that is used to determine the weight of each training example.
In other words, the kernal function is the function K such that $$w_i = K(d(x_i, x_q))$$

\textbf{Locally weight regression}

We can combine KNN and regression by:

\begin{itemize}
    \item Minimize squared error on the K nearest neighbors \\
    \item Minimize squared error on all examples, but use some kernal function to weight examples. \\
    \item Combine both, weight K nearest neighbors with a kernal function \\
\end{itemize}

In most cases the functon is approximated by a constant, linear, or quadratic function.
This is because the cost of approximating a more complex function for each instance
is too high, and simple approximations normally do a good job for a sufficiently
small subregion.

\textbf{Gaussian Kernal:}
$$K(x_a, x_b) \equiv e^{\frac{-D(x_a - x_b)}{2\sigma^2}}$$
We can apply a gaussian to R data points, add these gaussians
together, the divide this gaussian by n and we get a pdf for
predicting y given x\\
By changing $\sigma$ on the gaussians we can get kernal smoothing.


\textbf{Radial Basis Functions}

RBF netowrks are a type of ANN constructed from spatially localized
kernal function. These can be seen as a blend of instance-based
approaches (spatially localized influence of each kernal function) and
neural network approaches (a global approximation to the
target function is formed at training time rather than a local
approximation
at query time). RBF networks have been used successfully in applications such as interpreting
visual scenes, in which the assumption of spatially
local influences is well justified.

\textbf{Remarks on Lazy and eager learning:}

Lazy methods normally require low computation time during learning
but high computation at query time.
Lazy methods consider each query when deciding how to generalize beyond the
training examples, eager methods cannot do this. Before seeing the any queries
the eager methods choose a global generalization.

This means that lazy methods use a more expressive hypothesis space. 
Local methods can represent a global approximation by a collection of 
small approximations. 

\end{question}

\begin{question}{Genetic Algorithms}
Good things about GA:
\begin{itemize}
    \item Evolution is know to be successful\\
    \item GAs can search spaces of hypothesis containing complex interacting
    parts, where the impact of each part on overall hypothesis fitness may be difficult
    to model \\
    \item GA are easily parallelized\\
\end{itemize}
Elements of a GA:
\begin{itemize}
    \item \textbf{Representation}: A way to represent a hypothesis (normally a bitstring)\\
    \item \textbf{Population}: a collection of hypothesis \\
    \item \textbf{Fitness function}: A function that evaluates the quality of a hypothesis\\
    \item \textbf{Crossover operation}: Given two parents, a crossover operation creates
                        two children. The crossover operation should only produce
                        valid children\\
    \item \textbf{Mutation}: The way in which hypothesis can mutate, and the rate at which
                            mutate.\\
    \item \textbf{Rate of replacement}: A rate that determines how much of the population
                            will be removed at each generation.
\end{itemize}

TODO: Soft/Hard bias of GA
\end{question}

\begin{question}{Reinforcement Learning}

We want to learn the \textbf{control policy} which is a function from
the state space to actions: $$ \pi : S \rightarrow A$$
which outputs an appropriate action $a \in A$ given a state $s \in S$

Reinforcement learning differs from other function approximations in:
\begin{itemize}
    \item \emph{Delayed Reward:} For normal functions we having training
        data of the form $(d, f(d))$, but in reinforcement learning this
        is not the case.  This introduces the problem of \emph{temporal credit
        assignment}: determining which of the actions in its sequence are
        to be credited with producing the eventual rewards.\\
    \item \emph{Exploration:} The learner faces a tradeoff in choosing
        whether to favor exploration of unknown
        states and actions (to gather new information), or exploitation of states
        and actions that it has already learned will yeild high reward.\\
    \item \emph{Partially observable states:} May not be able to completely determin the state\\
    \item \emph{Lifelong learning: Learning never ends} 
\end{itemize}

Cumulative value with respect to an arbitrary policy $\pi$:

$$ V^{\pi}(s_t) = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} ..$$

This equation is normally called the \emph{discounted cumulative reward}

Instead of the discounted cumulative reward we could use the \emph{finite horizon}
$\sum^h r_{t+i}$ or the \emph{average reward} $lim_{h\rightarrow \infty} \frac{1}{h} \sum^h r_{t+i}$

The task of learning is to find the \emph{optimal policy} $\pi^*$
$$ \pi^* \equiv \arg\max_{\pi}V^{\pi}(s), (\forall s)$$

We denote $V^{\pi^*}$ as $V^*$

\textbf{Q-Learning}

Each equation follows from the previous.
$$Q(s,a) \equiv r(s,a) + \gamma V^*(\delta(s,a))$$
$$ \pi^*(s) \equiv \arg\max_{a}Q(s,a)$$
$$V^*(s) = \max_{a'} Q(s, a')$$
$$\hat{Q}(s,a) \leftarrow r + \gamma \max_{a'}\hat{Q}(s', a')$$


\end{question}


\end{document}
